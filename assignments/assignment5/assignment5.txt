------------
Assignment 5
------------

This assignment is due Tuesday, May 16th at 23:59:59 latest via email to

christian.wallraven+AMS2017@gmail.com

I am fine with one email per team. In your email you HAVE to specify the names and student IDs of ALL team members. Zip all code and files you need for letting the program run and name the resulting zip-file in the following way:

<STUDENTID1>_<STUDENTID2>_<...>_A5.zip

Files and submissions that DO NOT have this pattern WILL NOT BE GRADED! Take care to use the correct IDs as these are my way of assigning the scores to individuals!

Since almost all of you will be working in teams, I hope that you split the duties equally. Everyone should code a bit and - most importantly - everyone HAS to FULLY understand EVERY line of code AND be able to EXPLAIN THE CODE TO ME!!!




Duking it out - Naive Bayes versus Decision Trees [60 points]:
--------------------------------------------------------------

We are going to test who wins in this battle of the simple classifiers!

Make a script called NB_versus_DT.py, in which you test the power of Naive Bayes versus simple Decision Trees in predicting the correct class on both the IRIS data and the WINE data.

For this, use the previously provided examples of how to load the databases. You can use either numpy access or pandas DataFrames in the following.

The script should implement four helper functions:

def shuffleIrisIndices(percentage)
takes as input the percentage of each of the 3 IRIS classes to use as TRAINING. Returns the training indices and testing indices for the three classes as two numpy arrays

def shuffleWineIndices(percentage)
takes as input the percentage of each of the 3 wine classes to use as TRAINING. Returns the training indices and testing indices for the three classes as two numpy arrays

def runOneSplitNaiveBayes(trainingData,testData)
runs the full Naive Bayes implementation for one split of the data into training and testing. The input consists of the actual trainingData and testData, NOT the indices! Slides 22 and 23 of Chapter4_CW.pdf should help you in getting your memory refreshed about how to use numerical values in order to estimate the mean and standard deviation of the classes. Returns the ERRORS that the classifier makes as PERCENTAGE of len(testData). 
Note: this function needs to work on BOTH the IRIS AND the WINE data, so make sure that your data structure is set up properly!

def runOneSplitDecisionTree(trainingData,testData,max_depth=None)
runs a simple Decision Tree implementation for one split of the data into training and testing. The input consists of the actual trainingData and testData, NOT the indices! In addition the parameter max_depth is handed to the function that is used to initialize the tree. Check the class examples for how to construct a tree. Use "entropy" as the input to the constructor. Returns the ERRORS that the classifier makes as PERCENTAGE of len(testData). 
Note: this function needs to work on BOTH the IRIS AND the WINE data, so make sure that your data structure is set up properly!


In the main part of the script, run a dual for-loop for each the IRIS data and the WINE data that tests different percentages of training and test split and uses 20 repetitions for each split to estimate a better error.

# roughly like this:
percentages= [from 5 to 95]
reps = 20
for p in percentages:
    for i in reps:
    	errorsNB=runOneSplitNaiveBayes(trainingData,testData)
		errorsDT=runOneSplitDecisionTree(trainingData,testData)
		

Plot the result as a very nice plot that has the percentages on the x-axis and for each percentage the mean errors and their confidence intervals around it!

Which method is better and by how much? 


What are the storage requirements for the tree versus Naive Bayes in one split? Try to decrease max_depth so that you get a tree that is comparable in size to the Naive Bayes storage requirements and repeat the experiment from above, making another plot. 

Which method is better now and by how much? 

Insert all analyses as COMMENTS in the code. Also remember to COMMENT the code ITSELF!!!