------------
Assignment 4
------------

This assignment is due Wednesday, May 3rd at 23:59:59 latest via email to

christian.wallraven+AMS2017@gmail.com

I am fine with one email per team. In your email you HAVE to specify the names and student IDs of ALL team members. Zip all code and files you need for letting the program run and name the resulting zip-file in the following way:

<STUDENTID1>_<STUDENTID2>_<...>_A4.zip

Files and submissions that DO NOT have this pattern WILL NOT BE GRADED! Take care to use the correct IDs as these are my way of assigning the scores to individuals!

Since almost all of you will be working in teams, I hope that you split the duties equally. Everyone should code a bit and - most importantly - everyone HAS to FULLY understand EVERY line of code AND be able to EXPLAIN THE CODE TO ME!!!




First task [10 points]:
-----------------------


First, solve the DIY-IRIS task, where the goal is to replicate the PCA analysis for the wine-data using the flower data. Call this first script iris.py.

Load the data, plot the pairplot and add comments in the code as to which dimensions are correlating highly. Then follow the recipe for the wine-data and do the PCA, retaining the top two components and plotting the data in that new space using seaborn's lmplot as indicated in the script fragment. Insert a comment in the script that explains what you can see in this new plot.


Second task [10 points]:
------------------------

Second, the naive Bayes implementation of the weather data. Call this second script weather_bayes.py. 

Load the data, copy the stats part where I fill bayesData with the likelihoods into the script and implement the testing function testDay exactly as shown in the fragment. Make sure that it returns the tuple exactly as shown in class! 

Next, implement a simple for-loop (hint: you need a special part of pandas to easily iterate over rows in the data!) that returns the results for all rows of "data". How many errors do you make? So, is naive Bayes better than 1R?? Insert these observations as comments into your script.


Third task [60(!) points]:
--------------------------

Your third task will be to implement naive Bayes as a spam filter. I've uploaded 10 directories to the assignment4 folder that contain both "ham" and "spam" emails. The messages in each folder that start with "spm" are spam messages, the other ones are "ham" messages. The data is an excerpt from (see the readme.txt)

_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*
This directory contains the Ling-Spam corpus, as described in the 
paper:

I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, George Paliouras, 
and C.D. Spyropoulos, "An Evaluation of Naive Bayesian Anti-Spam 
Filtering". In Potamias, G., Moustakis, V. and van Someren, M. (Eds.), 
Proceedings of the Workshop on Machine Learning in the New Information 
Age, 11th European Conference on Machine Learning (ECML 2000), 
Barcelona, Spain, pp. 9-17, 2000.
_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*


We are going to use these 10 directories in a 10-fold cross-validation scheme (we will talk about this later in class - for now, just follow the instructions).

Write a script called ham_spam.py that implements the naive Bayes spam filter. For this:

- do the following 10 times for all possible splits of 10 folders into 9 folders and 1 folder (yes, there are 10 such splits!):

* use 9 of the 10 folders as training and the 10th folder as testing
* for all ham-files in the 9 training folders, load them as text, strip the non-letter characters (just like for the Shakespeare assignment), put all the words of all ham-files into one large string array and from this array calculate the ham-unigram frequencies (normalize by the total number of words in all ham files!)
* for all spam-files in the 9 training folders, load them as text, strip the non-letter characters (just like for the Shakespeare assignment), put all the words of all spam-files into one large string array and from this array calculate the spam-unigram frequencies (normalize by the total number of words in all spam files!)
* for all ham-files in the testing folder, load them as text, strip the non-letter characters, and calculate the likelihood that this email is ham by adding the logarithms of the ham-likelihoods for the found words - if a word is NOT present in the ham-list, use p=log(1/numHamWords). Next calculate the likelihood that it is spam in the same way. For each ham-email, choose whether it is ham or spam based on the maximum likelihood.
* for all spam-files in the testing folder, load them as text, strip the non-letter characters, and calculate the likelihood that this email is ham by adding the logarithms of the ham-likelihoods for the found words - if a word is NOT present in the ham-list, use p=log(1/numHamWords). Next calculate the likelihood that it is spam in the same way. For each spam-email, choose whether it is ham or spam based on the maximum likelihood.
* calculate how many ham-emails are wrongly classified as spam and how many spam-emails are correctly classified as spam

- average the spam false alarm rates and the spam hit rates over the 10 runs and output the results

If everything worked fine, you should get reasonably low false alarms and high hit rates. 

Check four mistake emails and try to tell me WHY they were misclassfied!

Check your classifier with 10 of your own "ham" and "spam" messages - does it work? Why? Why not?


*****Useful tips:******

(1) as you see from my description above, the treatment of ham and spam both for training and testing of the Bayes is very similar - I suggest you write a function!

(2) glob is useful for file operations - the following code snippet may come in handy:

import glob
for d in range(1,10):
	for file in glob.glob('part{:d}/*-*.txt'.format(d)):
	

(3) defaultdict is a dictionary to which you can simply add new keys, but a pandas DataFrame could also handle the job:

from collections import *
unigram = defaultdict(Counter)


(4) if you want to keep appending data to a string array, use:

data.extend(tmp)








