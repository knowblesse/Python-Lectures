{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree (categorical)\n",
    "\n",
    "Let's try to construct a decision tree for the weather data. If we stick to `scikit`, unfortunately, we already hit a big limitation of its decision tree implementation:\n",
    "\n",
    "* `scikit` decision trees are only for **numeric** data!!\n",
    "\n",
    "Our weather data, however, is categorical, so that we now need to do the attribute encoding that was discussed earlier.\n",
    "\n",
    "We **cannot** simply replace strings as values (i.e., \"Sunny\" = 1, \"Rainy\" = 2, etc.), since `scikit` actually treats these values as numbers, but our data has **no** ordering. If our data would be **ordinal**, we could do this, since that would make sense (i.e., \"Worst\" = -2, \"Neutral\" = 0, \"Best\" = 2, for example).\n",
    "\n",
    "So, we have to encoding our values in a **one-hot encoding**. For this, we can use a variety of approaches. Let's do `pandas` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in our weather data\n",
    "rawData = pandas.read_csv('data/dataWeather.txt',delimiter='\\t')\n",
    "\n",
    "# the labels are in the last column\n",
    "label = rawData['Play']\n",
    "# the actual \"data\" is in all other columns\n",
    "tmp = rawData.iloc[:,0:-1]\n",
    "\n",
    "# now convert the data to one-hot encoding!\n",
    "data = pandas.get_dummies(tmp)\n",
    "\n",
    "# construct a decision tree model using the ID information gain\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# fit it to our data\n",
    "dt.fit(data,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have constructed a tree and fit it to our weather data. \n",
    "\n",
    "We would now like to test that tree with a new day (e.g., \"Sunny\", \"Cool\", \"High\", and \"True\") to see whether we should play or not.\n",
    "\n",
    "Unfortunately, in order to test this, we also need to convert that day into a one-hot encoded input vector. But how can we do this? \n",
    "\n",
    "As always, there is a genius answer: we do the one-hot encoding for our original data, we do another for the new day, and then we **reindex** the resulting dummy variable with those coming from the original encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test with a new day\n",
    "\n",
    "# first do the one-hot encoding for the new day\n",
    "tmp = pandas.get_dummies(pandas.DataFrame({'Outlook':['Sunny'],'Temperature':['Cool'],'Humidity':['High'],'Windy':[True]}))\n",
    "\n",
    "# now re-index this with the original encoding, making sure to \n",
    "# add \"0\" to every column that does NOT appear in our tmp variable!\n",
    "newDay=tmp.reindex(columns = data.columns, fill_value=0)\n",
    "\n",
    "# finally, we can predict the decision:\n",
    "print('on the tested day, the decision to Play is:',dt.predict(newDay)[0],'with',np.max(dt.predict_proba(newDay)),'probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the tree using `export_graphviz` and `pydotplus`. We also select a few options to make a nicer, readable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "# export the tree using the correct feature names\n",
    "# colored by purity\n",
    "dot_data = tree.export_graphviz(dt,out_file=None,\n",
    "                                feature_names=data.columns,\n",
    "                                rounded=True,filled=True)\n",
    "# convert this to a picture structure\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "# show this picture as a PNG in the browser\n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the tree does select the Outlook attribute first, followed by Humidity. Since this is a binary tree, however, the splits do look different than the one in the powerpoint material. \n",
    "\n",
    "In general, the `scikit` implementation leaves a bit to be desired. If you want more control over your decision trees, you should download a more advanced package, such as:\n",
    "\n",
    "`Python Decision Tree 3.4.3` at https://engineering.purdue.edu/kak/distDT/DecisionTree-3.4.3.html\n",
    "\n",
    "## Training error\n",
    "\n",
    "Finally, let's take a look at the training error. Since we constructed the tree by default to keep splitting until all leaves are \"pure\", it will always produce 0 error. As we will see, especially for big problems, this may lead to \"overfitting\". One way to avoid this is to reduce the depth of the tree.\n",
    "\n",
    "We also construct a tree of depth 2 and see its error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('got',np.size(np.where(dt.predict(data)!=label)),'training errors for the full tree')\n",
    "\n",
    "# construct a decision tree model using the ID information gain\n",
    "dtSmall = tree.DecisionTreeClassifier(criterion='entropy',max_depth=2)\n",
    "\n",
    "# fit it to our data\n",
    "dtSmall.fit(data,label)\n",
    "\n",
    "print('got',np.size(np.where(dtSmall.predict(data)!=label)),'training errors for the small tree')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree for numeric data\n",
    "\n",
    "Let's use `scikit` to train a tree on numeric data. Let's first use a very simple two point example with a point at (0,0) that belongs to \"classA\" and another point at (1,1) that belongs to \"classB\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make simple data\n",
    "data = [[0,0],[1,1]]\n",
    "# give the points a label\n",
    "label = ['classA','classB']\n",
    "\n",
    "# construct a decision tree model using the ID information gain\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# fit it to our data\n",
    "dt.fit(data,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've fit the tree, let's test this with two nearby points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test0 = dt.predict([[1.3,1.3]])\n",
    "test1 = dt.predict([[0.1,0.2]])\n",
    "\n",
    "print('predicted {:s} for first point and {:s} for second'.format(test0[0],test1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the decision space of the tree in x,y coordinates. For this, we first construct a `meshgrid` of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x,y)=(np.linspace(-0.5,1.5,21),np.linspace(-0.5,1.5,21))\n",
    "(xv,yv)=np.meshgrid(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, while this is very good for function evaluation and plotting, the format of `meshgrid` is not good for the `predict` method of the decision tree in `scikit`. \n",
    "\n",
    "So, we need to combine the coordinates into pairs of x,y, which is done by the `ravel` command from `numpy`. The output of this is converted to a `numpy` array and fed in the tree predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = dt.predict(np.array([xv.ravel(), yv.ravel()]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will draw the decision space by substituting \"0\" for \"classA\" and \"1\" for \"classB\". The result needs to be reshaped into the original `meshgrid` format and then we can plot the contours of the different classification outputs as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[test=='classA']=0\n",
    "test[test=='classB']=1\n",
    "test = test.reshape(xv.shape)\n",
    "fig=plt.figure()\n",
    "plt.contourf(xv, yv, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the plane is split into two sections [whether this split is horizontal or vertical is random!]\n",
    "\n",
    "Let's take a look at the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "# export the tree colored by purity\n",
    "dot_data = tree.export_graphviz(dt,out_file=None,\n",
    "                                feature_names=['X','Y'],\n",
    "                                rounded=True,filled=True)\n",
    "# convert this to a picture structure\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "# show this picture as a PNG in the browser\n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, well...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR-tree\n",
    "\n",
    "Let's try to see what the famous XOR problem does to the tree: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make simple data\n",
    "data = [[0,0],[1,1],[0,1],[1,0]]\n",
    "# give the points a label\n",
    "label = ['classA','classA','classB','classB']\n",
    "\n",
    "# construct a decision tree model using the ID information gain\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# fit it to our data\n",
    "dt.fit(data,label)\n",
    "\n",
    "# test it with a range of coordinates\n",
    "test = dt.predict(np.array([xv.ravel(), yv.ravel()]).T)\n",
    "test[test=='classA']=0\n",
    "test[test=='classB']=1\n",
    "test = test.reshape(xv.shape)\n",
    "fig=plt.figure()\n",
    "plt.contourf(xv, yv, test)\n",
    "\n",
    "# export the tree colored by purity\n",
    "dot_data = tree.export_graphviz(dt,out_file=None,\n",
    "                                feature_names=['X','Y'],\n",
    "                                rounded=True,filled=True)\n",
    "# convert this to a picture structure\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "# show this picture as a PNG in the browser\n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the tree correctly classifies the problem. It can only do this by having a replicated sub-tree.\n",
    "\n",
    "In general, such trees can carve arbitrary boxes out of the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY: Decision tree on the IRIS data\n",
    "\n",
    "Let's try the IRIS data with our decision tree. This is purely numerical, so the `scikit` implementation has no problem. We will first load the data and then select a sub-set for training and another for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load iris data\n",
    "iris = \n",
    "\n",
    "# we know that the data has 150 flowers, 50 flowers for each\n",
    "# of the three categories. I would like to get a subset of \n",
    "# each flower for training, and the remainder for testing\n",
    "\n",
    "# select the first 40 flowers of each category for training\n",
    "trainIdx = \n",
    "# select the remaining 10 flowers of each category for testing\n",
    "testIdx = \n",
    "\n",
    "# construct the default tree\n",
    "dtIris = \n",
    "\n",
    "# fit it to our training data\n",
    "dtIris = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now let's predict the results on the test data and evaluate how good we are. \n",
    "\n",
    "Let's also visualize the tree (see above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = dtIris.predict(iris.data[testIdx])\n",
    "print('got',np.size(np.where(test!=iris.target[testIdx])),'training errors for the tree')\n",
    "\n",
    "# export the tree colored by purity\n",
    "dot_data = tree.export_graphviz(dtIris,out_file=None,\n",
    "                                feature_names=iris.feature_names,\n",
    "                                class_names=iris.target_names,\n",
    "                                rounded=True,filled=True)\n",
    "# convert this to a picture structure\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "# show this picture as a PNG in the browser\n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of this is that we can actually see that one full class (\"setosa\") can be predicted by just asking one simple question on the \"petal length\". Separating the \"virginica\" from the \"versicolor\" is a little bit more involved.\n",
    "\n",
    "Note, that we've trained on only a subset of the data and tested on another, independent, test set. We will talk about this a lot more soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
