{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import sys\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import csv\n",
    "import datetime\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "The following is taken from the excellent tutorial on RNNs and adapted a bit.\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n",
    "\n",
    "Our goal is to learn dependencies between words from a corpus given a recurrent neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Reading in the corpus\n",
    "\n",
    "In the following, we are going to use a Shakespeare corpus (that only includes his plays, not the sonnets) to train our RNN. \n",
    "\n",
    "In order to get our network to properly train sentences, we are going to prepend sentence start tokens and append sentence end tokens to each of the corpus's sentences.\n",
    "\n",
    "We are going to make use of the natural language toolkit to help with some processing.\n",
    "\n",
    "In order to get this, you need to `pip3 install nltk` and then type from a python console `ntlk.download()` and select the popular packages for downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabularySize = 8000\n",
    "unknownToken = \"UNK\"\n",
    "sentenceStartToken = \"S_START\"\n",
    "sentenceEndToken = \"S_END\"\n",
    " \n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print(\"Reading shakespeare plays...\")\n",
    "with open('data/shakespeare_plays.txt') as f:\n",
    "    # split the data into sentences using nltk magic\n",
    "    # this simply iterates through each line, lowercases it, and then\n",
    "    # sends it to nltk, which splits the lines into sentences according\n",
    "    # to English punctuation rules (i.e., .!?) and also handles problem cases\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x.lower()) for x in f])\n",
    "    # for each of the sentences we prepend S_START and\n",
    "    # and append S_END tokens for our network to learn\n",
    "    sentences = [\"%s %s %s\" % (sentenceStartToken, x, sentenceEndToken) for x in sentences]\n",
    "print(\"read {:d} sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we are going to use ntlk to tokenize our sentences - tokenize in this case means to split it into words and sentence markers (such as ,.! and so on). In our case, we would like to get words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now get words from the sentences - nltk uses much smarter methods\n",
    "# to do this than our crude regular expressions from the first assignment\n",
    "# but: if also takes a lot longer...\n",
    "tokenizedSentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    " \n",
    "# now we determine the unigram\n",
    "unigram = nltk.FreqDist(itertools.chain(*tokenizedSentences))\n",
    "print (\"found {:d} unique words\".format(len(unigram.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2: Preparing the data structures\n",
    "\n",
    "The RNN will not directly work on words, but will process each word as its index in the corresponding unigram of the text. \n",
    "\n",
    "So, we will use nltk to give us the unigram and the indices, and then create a variable `index2word` and `word2index` that will convert back and forth between the two representations.\n",
    "\n",
    "The RNN will be trained on the output of `word2index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use only the vocabularySize most common words - yes, nltk has a\n",
    "# helper function for that, too ^^\n",
    "vocab = unigram.most_common(vocabularySize-1)\n",
    "# create index2word and word2index indexing vectors that we need later\n",
    "index2word = [x[0] for x in vocab]\n",
    "# don't forget our unknown token:\n",
    "index2word.append(unknownToken)\n",
    "# and reverse index2word:\n",
    "word2index = dict([(w,i) for i,w in enumerate(index2word)])\n",
    " \n",
    "print(\"We choose a vocabulary size of {:d}\".format(vocabularySize))\n",
    "print(\"The least frequent word in the vocabulary is '{:s}' and appeared {:d} times.\".format(vocab[-1][0], vocab[-1][1]))\n",
    "print(\"The most frequent non start/end token word in the vocabulary is '{:s}' and appeared {:d} times.\".format(vocab[2][0], vocab[2][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that we restricted our vocabulary to 8000, so the unigram will contain words that our not in our dictionary. \n",
    "\n",
    "We have to replace these words by the UNK token in order to deal with them during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now we need to go back and replace all words that are NOT\n",
    "# in the vocabulary with the UNK token!\n",
    "for i, sent in enumerate(tokenizedSentences):\n",
    "    tokenizedSentences[i] = [w if w in word2index else unknownToken for w in sent]\n",
    "    \n",
    "print(\"\\nExample sentence: '{:s}'\".format(sentences[0]))\n",
    "print(\"\\nExample sentence after Pre-processing:\",tokenizedSentences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Create training data\n",
    "\n",
    "This is the easiest part. We simply get all the words and put them into an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create training data for the network\n",
    "Xtrain = np.asarray([[word2index[w] for w in sentence[:-1]] for sentence in tokenizedSentences])\n",
    "ytrain = np.asarray([[word2index[w] for w in sentence[1:]] for sentence in tokenizedSentences])\n",
    "\n",
    "# show examples sentences for X and y \n",
    "for i in Xtrain[40]:\n",
    "    print(index2word[i])\n",
    "for i in ytrain[40]:\n",
    "    print(index2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: The RNN\n",
    "\n",
    "Now for the actual beef of this. In the following, we will create an RNN-class that handles all the training for us.\n",
    "\n",
    "The reason for doing a class is that this nicely capsulates all the functionality for us. Also, all current python interfaces to neural network tools (such as tensorflow, Theano, etc.) use class-based approaches, so that we can get some feeling here as well.\n",
    "\n",
    "### Step 4.1: Initialization\n",
    "Let's first add the constructor to the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # the constructor of the class - this function gets automatically \n",
    "    # called if you make a new instance of the class\n",
    "    \n",
    "    # the first variable is the name under which you refer to the\n",
    "    # current instance itself and is usually named \"self\"\n",
    "    \n",
    "    # wordDim:      the number words in our dictionary\n",
    "    # hiddenDim:    the number of hidden states or \"memory\"\n",
    "    # bpttTruncate: the number of time-steps we can go back\n",
    "    def __init__(self, wordDim, hiddenDim=100, bpttTruncate=4):\n",
    "        # assign the instance variables\n",
    "        self.wordDim = wordDim\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.bpttTruncate = bpttTruncate\n",
    "        # initialize the network weights with small random numbers\n",
    "        # there is a bit of \"witchcraft\" here in how you initialize these\n",
    "        # for now, let's just say that small, random values are fine, but that\n",
    "        # you can do better with [-1/sqrt(n),1/sqrt(n)]\n",
    "        self.U = np.random.uniform(-np.sqrt(1./wordDim), np.sqrt(1./wordDim), (hiddenDim, wordDim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim), (wordDim, hiddenDim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim), (hiddenDim, hiddenDim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 4.2: The forward propagation and the loss function\n",
    "Now let's define one forward pass through the RNN. For this, we need the output of the final layer, which is an array of probabilities for each of the words in our dictionary. \n",
    "\n",
    "There are many ways to define this, but a popular loss is the softmax-loss, which uses the softmax-function $S$ that takes as input a vector $\\vec{x}$ of dimension $d$ and returns as output:\n",
    "\n",
    "$S(\\vec{x}) = \\frac{e^{\\vec{x}}}{\\sum_{k=1}^{D}e^{x_k}}$\n",
    "\n",
    "One easy view of the effects of this function is that it stretches the contrast of the input, such that high elements of the vector in the input will be even higher in the output. The output is normalized to be between 0 and 1, and that gives us basically a probabilistic output!\n",
    "\n",
    "Because of numerical issues with the exponential function, large elements will lead to overflow (think $e^{1000}$), so people use the fact that the function $S(\\vec{x})=S(C\\vec{x})$ for an arbitrary $C$, which we can freely choose. In practice $C=max_k(x_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# output loss for the final layer\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now for the actual forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# one forward pass\n",
    "def forwardProp(self, x):\n",
    "    # how many time steps we have - this is the number of\n",
    "    # words in the sentence!\n",
    "    T = len(x)\n",
    "    # we save all hidden states in s, because we need them later\n",
    "    # we reserve one additional element for the initial hidden state\n",
    "    s = np.zeros((T + 1, self.hiddenDim))\n",
    "    \n",
    "    # the outputs for each time step - each time step will predict\n",
    "    # probabilities for all wordDim words!!!\n",
    "    o = np.zeros((T, self.wordDim))\n",
    "    # now go through each time step\n",
    "    for t in np.arange(T):\n",
    "        # the current hidden state is the activation function of\n",
    "        # the input, which is the previous hidden state multiplied\n",
    "        # by W and the addition of the current variable weighted by U\n",
    "        \n",
    "        # in order to multiply U with x[t], we would need to do some\n",
    "        # sort of one-hot encoding of x[t]! This would be done in \n",
    "        # standard NN packages\n",
    "        # but we can also use a trick: we can simply index into \n",
    "        # our weight array U with x[t], which will be the same \n",
    "        # as multiplying U with a one-hot vector\n",
    "        \n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "# we only do this here, since we left the \"class\" scope and are\n",
    "# doing an \"outside\" decleration of that class functionality\n",
    "RNN.forwardProp=forwardProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 4.3: Predicting something with the RNN\n",
    "The output of the RNN is a large vector of probabilities, with one element for each of our dictionary words.\n",
    "\n",
    "So, if we want to predict the \"best\" word, we simply need to select the index with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # do forward propagation and the index of the highest score\n",
    "    o, _ = self.forwardProp(x)\n",
    "    return np.argmax(o, axis=1)\n",
    " \n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing the forward pass\n",
    "\n",
    "Let's choose our example sentence and push it through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# init\n",
    "model = RNN(vocabularySize)\n",
    "# do one pass with an example sentence that has 11 words\n",
    "o, s = model.forwardProp(Xtrain[40])\n",
    "# so our output will have probabilities for each of the 11 words for\n",
    "# all entries in our dictionary\n",
    "print(o.shape)\n",
    "print(o)\n",
    "# here are the hidden states [we have one more to model the initial state]\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These probabilities are completely random, of course, since we actually have not trained the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predicting something with the model\n",
    "Let's use that example sentence to predict the words that we would get and compare that to the actual words that we wanted to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(Xtrain[40])\n",
    "print(predictions.shape)\n",
    "print(predictions)\n",
    "for idx,i in enumerate(predictions):\n",
    "    print('predicted:',index2word[i],' wanted:',index2word[ytrain[40][idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see, we don't have much of a match, which is understandable, since all the weights are currently random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 4.4 Determine loss\n",
    "\n",
    "The final layer calculates the output, which consists of the probabilities for each word in the dictionary. Now, to determine the full loss, we need to of course do this **for each** of the sentences and then sum up the log of the probabilities to keep things small and easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculateTotalLoss(self, x, y):\n",
    "    # initialize loss\n",
    "    L = 0\n",
    "    # go through each sentence of the target\n",
    "    for i in np.arange(len(y)):\n",
    "        # do a forward pass and get the output (and the state for which we do not care here)\n",
    "        o, _ = self.forwardProp(x[i])\n",
    "        # now we check the output for those words that are contained\n",
    "        # in the target sentence:\n",
    "        correctWordProbabilities = o[np.arange(len(y[i])), y[i]]\n",
    "        # this tells us how wrong we are, so we add that to the loss\n",
    "        L += -1 * np.sum(np.log(correctWordProbabilities))\n",
    "    return L\n",
    "\n",
    "# this one is easy, we just some up all the words we have in the \n",
    "# target set \n",
    "def calculateLoss(self, x, y):\n",
    "    # total loss divided by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculateTotalLoss(x,y)/N\n",
    " \n",
    "RNN.calculateTotalLoss = calculateTotalLoss\n",
    "RNN.calculateLoss = calculateLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's test this loss. Running this on the full dataset will be very slow, so here we only use a subset to show that our initial loss is close to the random prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Expected Loss for random predictions: {:f}\".format(np.log(vocabularySize)))\n",
    "# running this on the full data set is prohibitively slow\n",
    "# so just use the first 100 to \"approximate\" \n",
    "print(\"Actual loss: {:f}\".format(model.calculateLoss(Xtrain[:100], ytrain[:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 4.5: Backpropagation through time (BPTT)\n",
    "\n",
    "Here comes the second piece of magic and the core of RNNs: the back-propagation through time.\n",
    "\n",
    "If you remember our discussion on training standard multi-layer networks, you also remember backpropagation.\n",
    "\n",
    "Here, the algorithm is exactly the same with the added twist that time gets unfolded - note that we would need to do this for the full length of the training data! \n",
    "\n",
    "The actual weight updates are calculated in the exact same way as for the standard backprop algorithm.\n",
    "\n",
    "You initialize with the output of the last layer and then push back through all hidden layers.\n",
    "\n",
    "Note that a full understanding of this algorithm requires the calculation of a rather lengthy set of derivatives. For a full explanation and derivation, see here:\n",
    "https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf\n",
    "\n",
    "\n",
    "Note also that there are many more methods to implement this, including shortening the loop:\n",
    "https://gist.github.com/m-liu/36c5e78d98c92fd8fa0c0f1efe2269c4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    # number of \"time steps\" - again this is the length of the sentence!\n",
    "    T = len(y)\n",
    "    # push everything through the network once\n",
    "    o, s = self.forwardProp(x)\n",
    "    # these store the gradients\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    # final layer initialization: \n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # now we take a look at each output\n",
    "    # and go backwards through each word in the sentence\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        # update of loss with respect to hidden node\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # now calculate first delta\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # now unfold network and go back through time \n",
    "        # we do a maximum of self.bpttTruncate time travel steps\n",
    "        # the max's are there to avoid boundary effects\n",
    "        # the higher this parameter, the better in \"theory\"\n",
    "        for bpttStep in np.arange(max(0, t-self.bpttTruncate), t+1)[::-1]:\n",
    "            #print(\"bptt: step t={:d} bptt step={:d} \".format(t, bpttStep))\n",
    "            # update W\n",
    "            dLdW += np.outer(delta_t, s[bpttStep-1])\n",
    "            # update U\n",
    "            dLdU[:,x[bpttStep]] += delta_t\n",
    "            # change delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bpttStep-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNN.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 4.6: Make training\n",
    "\n",
    "In order to demonstrate the BPTT, we implement a function that only does one update step of all weights.\n",
    "\n",
    "This is a very simple gradient descent update on all three parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gd1step(self,Xtrain,ytrain,learningRate):\n",
    "    # first calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(Xtrain, ytrain)\n",
    "    # then change parameters according to gradients and learning rate\n",
    "    self.U -= learningRate * dLdU\n",
    "    self.V -= learningRate * dLdV\n",
    "    self.W -= learningRate * dLdW\n",
    "    \n",
    "RNN.gd1step = gd1step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now for the actual training. Here we put in the gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gradient descent training\n",
    "# - model: The RNN model instance\n",
    "# - Xtrain: The training data set [words x sentences]\n",
    "# - ytrain: The target training data [words [shifted by one] x sentences]\n",
    "# - learningRate: Initial learning rate for SGD\n",
    "# - nEpoch: Number of times to iterate through the complete dataset\n",
    "# - evaluateLossEvery: Evaluate the loss every epochs\n",
    "def train(model, Xtrain, ytrain, learningRate=0.005, nEpoch=100, evaluateLossEvery=5):\n",
    "    # keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    numExamplesSeen = 0\n",
    "    # loop through all epochs\n",
    "    for epoch in range(nEpoch):\n",
    "        # do we want to evaluate the loss?\n",
    "        # caution: this can be expensive!\n",
    "        if (epoch % evaluateLossEvery == 0 and epoch>0):\n",
    "            loss = model.calculateLoss(Xtrain, ytrain)\n",
    "            losses.append((numExamplesSeen, loss))\n",
    "            time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"{:s}: Loss after numExamplesSeen={:d} epoch={:d}: {:f}\".format(time, numExamplesSeen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            #if (len(losses) >= 1 and losses[-1][1] >= losses[-2][1]):\n",
    "            #    learningRate = learningRate * 0.5 \n",
    "            #    print(\"Setting learning rate to {:f}\".format(learning_rate))\n",
    "            sys.stdout.flush()\n",
    "        # for each training sentence...\n",
    "        for i in range(len(ytrain)):\n",
    "            # do one gradient step\n",
    "            model.gd1step(Xtrain[i], ytrain[i], learningRate)          \n",
    "            numExamplesSeen += 1\n",
    "            if (i%1000 ==0):\n",
    "                print(numExamplesSeen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 5: Testing\n",
    "\n",
    "Let's test how fast one step is for one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "model = RNN(vocabularySize)\n",
    "%timeit model.gd1step(Xtrain[40], ytrain[40], 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ouch. Now imagine that for one whole epoch, where we have 160K sentences, which means roughly one hour to go through our full training set! In order to do good training, we should probably do a few 10s of epochs!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNN(vocabularySize)\n",
    "losses = train(model, Xtrain[:10], ytrain[:10], nEpoch=1000, evaluateLossEvery=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "indexToTest=5\n",
    "predictions = model.predict(Xtrain[indexToTest])\n",
    "print(predictions.shape)\n",
    "print(predictions)\n",
    "for idx,i in enumerate(predictions):\n",
    "    print('predicted:',index2word[i],' wanted:',index2word[ytrain[indexToTest][idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Much better :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generateSentence(model):\n",
    "    # since the model predict indices, we are going to get\n",
    "    # index values into a variable called newSentence\n",
    "    \n",
    "    # sentence beginning is defined by the sentenceStartToken\n",
    "    newSentence = [word2index[sentenceStartToken]]\n",
    "    # from this, we predict until we get sentenceEndToken\n",
    "    while not newSentence[-1] == word2index[sentenceEndToken]:\n",
    "        # that's the main part - here we use the model to predict\n",
    "        # the next words, given the current state\n",
    "        nextWordProbabilities,_ = model.forwardProp(newSentence)\n",
    "        # from this, we will sample a random word\n",
    "        sampledWord = word2index[unknownToken]\n",
    "        # iterate through list until we hit word that is not unknown\n",
    "        while sampledWord == word2index[unknownToken]:\n",
    "            samples = np.random.multinomial(1, nextWordProbabilities[-1])\n",
    "            sampledWord = np.argmax(samples)\n",
    "        newSentence.append(sampledWord)\n",
    "    # now convert the index into strings\n",
    "    sentence = [index2word[x] for x in newSentence[1:-1]]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# Train on a small subset of the data to see what happens\n",
    "modelBig = RNN(vocabularySize)\n",
    "losses = train(modelBig, Xtrain[:10000], ytrain[:10000], nEpoch=50, evaluateLossEvery=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numSentences = 10\n",
    "sentenceMinLength = 3\n",
    " \n",
    "for i in range(numSentences):\n",
    "    sentence = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sentence) <= sentenceMinLength:\n",
    "        sentence = generateSentence(modelBig)\n",
    "    print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Interpreting the results\n",
    "\n",
    "Example sentences do not look too promising, even though training proceeded for quite some time.\n",
    "\n",
    "In fact, this implementation does not suffer from computational resources (i.e., simply training longer would be better), but from a very fundamental issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in characters instead\n",
    "Our model is based on word prediction. What about predicting the next character instead?\n",
    "\n",
    "Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reading Shakespeare as full text\n",
    "allData = open('data/shakespeare_plays.txt', 'r').read()\n",
    "# splitting into characters done smartly making the use of sets/lists\n",
    "characters = list(set(allData.lower()))\n",
    "# checking the size of the data\n",
    "dataSize = len(allData)\n",
    "vocabularySize = len(characters)\n",
    "print('Shakespeare has {:d} characters in total with {:d} unique characters in the vocabulary.'.format(dataSize, vocabularySize))\n",
    "# creating dictionaries of indices and converting back\n",
    "character2Index = { ch:i for i,ch in enumerate(characters) }\n",
    "index2character = { i:ch for i,ch in enumerate(characters) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make the training data\n",
    "p=0\n",
    "# this will hold the sentences for training and testing \n",
    "XtrainChar=list()\n",
    "ytrainChar=list()\n",
    "# this is the number of characters we want to investigate\n",
    "scope = 25\n",
    "# now go through our data\n",
    "while p+scope+1<=dataSize:\n",
    "    # grab the current scope\n",
    "    i = [character2Index[ch] for ch in allData[p:p+scope].lower()]\n",
    "    XtrainChar.append(i)\n",
    "    # offset the scope by one\n",
    "    t = [character2Index[ch] for ch in allData[p+1:p+scope+1].lower()]\n",
    "    ytrainChar.append(t)\n",
    "    p=p+scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# Train on a small subset of the data to see what happens\n",
    "modelChar = RNN(vocabularySize)\n",
    "losses = train(modelChar, XtrainChar[:1000], ytrainChar[:1000], nEpoch=200, evaluateLossEvery=1,learningRate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred=modelChar.predict(XtrainChar[10])\n",
    "for idx,p in enumerate(pred):\n",
    "    print(\"p:\",index2character[p],\"  wanted:\",index2character[ytrainChar[10][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probs,_=modelChar.forwardProp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x=[character2Index[ch] for ch in ['']]\n",
    "string=list()\n",
    "string.append(index2character[1])\n",
    "for i in range(100):\n",
    "    probs,_=modelChar.forwardProp(x)\n",
    "    idx = np.random.choice(np.arange(84),1,p=probs[0])\n",
    "    x.append(idx[0])\n",
    "    string.append(index2character[idx[0]])\n",
    "print(''.join(string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
