{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Encoding of nominal variables to numerical variables\n",
    "Let's create a random set of 100 events that took place in one of three cities. The resulting data is one nominal variables with three entries. If we want to compute things with this, often it is useful to convert that nominal data into a number. \n",
    "\n",
    "Examples of this are one-hot encoding for text processing: \n",
    "* given a text, first you do the unigram of your dictionary and each word in the text is an index into that unigram array - let's say we find $n$ different words\n",
    "* let's say, the current word we are looking at can be found at index $i$ in the unigram\n",
    "* we can view this word in the text therefore as an $n$-dimensional vector, which is zero except for the $i$-th component (the one hot element in the otherwise cold vector!)\n",
    "\n",
    "Why do we use this (rather blown-up) representation?\n",
    "\n",
    "Many machine learning algorithms cannot handle nominal data natively and instead work better with feature vectors. Most trees as we have seen, however, can handle nominal data easily and hence, the one-hot encoding is NOT necessary. \n",
    "\n",
    "Incidentially, a variant of one-hot encodings called **dummy encoding** is also used in statistics to convert nominal (categorical) data into a format that can be handled in an easier way by statistics algorithms. Note, however, that they are **not** quite the same - the difference between these two types of encoding is especially tricky for regression models built on categorical data (we will not look into this here!)\n",
    "\n",
    "https://en.wikiversity.org/wiki/Dummy_variable_(statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a sample of unique values\n",
    "randVal = np.random.randint(low=0,high=3,size=100)\n",
    "randSeries = pandas.Series(randVal)\n",
    "# create a mapping dictionary\n",
    "mapper = {0: 'New York', 1: 'London', 2: 'Zurich'}\n",
    "# convert the number data to text\n",
    "nomvar = randSeries.replace(mapper)\n",
    "\n",
    "# with pandas.get_dummies, we can do the encoding:\n",
    "print(pandas.get_dummies(nomvar))\n",
    "\n",
    "# with sklearn.LabelEncoder, we can do this, too:\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# transform to numeric labels - note, we have to reshape this\n",
    "# array since that is required by the next stage\n",
    "labelEncoded = LabelEncoder().fit_transform(nomvar).reshape(-1,1)\n",
    "# transform '2D' array to binary encoding\n",
    "oneHotEncoded = OneHotEncoder().fit_transform(labelEncoded).toarray()\n",
    "print(oneHotEncoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Encoding of ordinal variables to numerical variables\n",
    "We can repeat the same idea with ordinal variables, but here it will be important to preserve their ordering. Hence, the mapping algorithms from pandas will not do the job properly and we have to generate our own mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a sample of unique values\n",
    "randVal = np.random.randint(low=0,high=3,size=100)\n",
    "randSeries = pandas.Series(randVal)\n",
    "# create a mapping dictionary, replacing values\n",
    "mapper = {1: 'small', 2: 'medium', 0: 'large'}\n",
    "ordvar = randSeries.replace(mapper)\n",
    "# this yields a numeric representation of the \"ordinal\" array\n",
    "pFact,pFactInd = pandas.factorize(ordvar)\n",
    "# if we execute the code multiple times, we will see\n",
    "# that pandas chooses the index randomly\n",
    "# --> order is not preserved!\n",
    "print(pFact,pFactInd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, this does not work. Let's do this ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# So, we basically do the same thing backwards\n",
    "\n",
    "# choose our own mapping algorithm to go from \n",
    "# ordinal data values to numbers\n",
    "preservingMapper = {'large':2 , 'medium': 1, 'small': 0}\n",
    "# and convert array\n",
    "ordvar.replace(preservingMapper)\n",
    "# now we are fine\n",
    "print(ordvar.replace(preservingMapper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Quick view on numerical data\n",
    "The file http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data contains data on concentrations of 13 different chemicals in wines grown in the same region in Italy that are derived from three different cultivations. Hence, for each wine we have 13 different numerical values, like this:\n",
    "\n",
    "`1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065`\n",
    "`1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050`\n",
    "`...`\n",
    "\n",
    "The first column is the wine type, and the following columns are the 13 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the data from the internets using pandas\n",
    "data = pandas.read_csv((\"http://archive.ics.uci.edu/ml/\"\n",
    "                        \"machine-learning-databases/wine/wine.data\"), header=None)\n",
    "\n",
    "# rename column names to “V1” .. “V14”\n",
    "data.columns=[\"V\"+str(i) for i in range(1,len(data.columns)+1)]\n",
    "\n",
    "# map the numbers to some nicer label\n",
    "labelMapper = {1:'Label1', 2:'Label2', 3:'Label3'}\n",
    "# and convert array (I have to reassign it!)\n",
    "data.V1=data.V1.replace(labelMapper)\n",
    "\n",
    "# print out subview of the data\n",
    "# (note that slicing of pandas DataFrame data directly\n",
    "# is done via the \".loc\" property)\n",
    "print(data.loc[0:10,\"V1\":\"V5\"])\n",
    "\n",
    "# The first column contains the 3 labels \n",
    "# we call this the \"dependent variable\"\n",
    "# Usually, some algorithm would try to \n",
    "# --- characterize the 13 different chemicals for\n",
    "#     each of the 3 labels\n",
    "# --- predict this label from the 13 different \n",
    "#     chemical dimensions in the remaining data\n",
    "y = data.V1\n",
    "\n",
    "# get all the chemical variables for each wine into X\n",
    "X = data.loc[:, \"V2\":]\n",
    "\n",
    "\n",
    "# to drive the point home that .loc indexing of DataFrames\n",
    "# has inclusive indexing:\n",
    "# this give us the first 11 rows\n",
    "numpyArray=np.array(X.loc[0:10,\"V2\":\"V5\"])\n",
    "# nope - we get only the first 10 rows!!\n",
    "print(numpyArray[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now let's take a look at the data using a scatter \n",
    "# matrix plot across the first 4 data dimensions\n",
    "pandas.tools.plotting.scatter_matrix(data.loc[:,\"V2\":\"V5\"],diagonal=\"hist\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Observations\n",
    "* We can see that the histograms (on the diagonal) for some values are decidedly skewed and hence we should be careful about some statistical statements (such as all those having to do with Gaussian/normal assumptions!)\n",
    "* It looks like there are some correlations in that data. For example, V4 versus V5. \n",
    "\n",
    "Given our look at the data, let's plot the correlation for V4 versus V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "# this makes sure that the axes are drawn BELOW the data\n",
    "ax.set_axisbelow(True)\n",
    "plt.grid()\n",
    "# loop through all labels\n",
    "for i in np.unique(data.loc[:,\"V1\"]):\n",
    "    # find out index belonging to each label\n",
    "    index=data.loc[:,\"V1\"]==i\n",
    "    # scatter this\n",
    "    plt.scatter(data.loc[index,\"V4\"],data.loc[index,\"V5\"],label=i)\n",
    "plt.xlabel('V4')\n",
    "plt.ylabel('V5')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Observations\n",
    "* there is a clear correlation between the two chemicals\n",
    "* there may be some discriminative power for the three labels going from the bottom right to the top left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ann = plt.annotate('discriminative\\ndirection?',xy=(2,27.5),xytext=(2.75,12.5),arrowprops=dict(arrowstyle='->'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PCA\n",
    "Could we try to get these directions (the correlation direction and the discriminative direction) from the data? \n",
    "\n",
    "Principal Component Analysis to the rescue. The two eigenvectors of PCA exactly describe these two directions for us.\n",
    "\n",
    "As a reminder: PCA rotates the original data coordinates such that the new axes point in the direction of maximum variances. The new axes are orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# get data\n",
    "pcaData = np.array(X.loc[:,\"V4\":\"V5\"])\n",
    "pcaDataM = pcaData.mean(axis=0)\n",
    "# standardize the data\n",
    "pcaDataStd = (pcaData-pcaDataM)\n",
    "# construct the method\n",
    "pca = PCA()\n",
    "# fit the method to the data\n",
    "pca.fit(pcaDataStd)\n",
    "# now the pca contains the results from\n",
    "# which we want the \"components_\" data\n",
    "vec1 = pca.components_[0]+pcaDataM\n",
    "vec2 = pca.components_[1]+pcaDataM\n",
    "plt.plot([pcaDataM[0],vec1[0]],[pcaDataM[1],vec1[1]],'k')\n",
    "plt.plot([pcaDataM[0],vec2[0]],[pcaDataM[1],vec2[1]],'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Wait a second! These do **not** seem orthogonal! What happened?\n",
    "\n",
    "Let's re-adjust the aspect ratio properly and check that the two components are indeed orthogonal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ann.remove()\n",
    "ax.set_aspect('equal')\n",
    "print('The dot product between the two components (should be 0) is: ',np.dot(pca.components_[0],pca.components_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Seaborn\n",
    "Python has an abundance of helper packages for visualization, plotting, data analysis, etc.\n",
    "\n",
    "One particularly nice one is seaborn which is a higher-level interface for matplotlib, allowing you to plot a few things in a \"nicer\" way.\n",
    "\n",
    "You can install it as usual with `pip3 install seaborn` and check some examples here:\n",
    "\n",
    "https://seaborn.pydata.org/examples/index.html\n",
    "\n",
    "To see how things are different compared to matplotlib, let's re-plot the scatter matrix of the wine-data with seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "sb.pairplot(data.loc[:,\"V1\":\"V5\"],hue=\"V1\",kind=\"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What seaborn shows here is:\n",
    "* the cumulative histograms on the diagonal for each of the three labels\n",
    "* the correlation scatter plots including standard, linear regression fits with confidence intervals\n",
    "\n",
    "We can see that V4 versus V5 has the clearest correlations.\n",
    "\n",
    "Let's look at the matrix of correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pandas has a correlation helper function for DataFrames:\n",
    "c = X.corr()\n",
    "# and seaborn has a nice visualization:\n",
    "fig, ax = plt.subplots(figsize=(8,8)) \n",
    "sb.heatmap(c, vmin=-1., vmax=1., square=True, annot=True, fmt='.2f', ax=ax).xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Actually, we can see a lot of correlations in our dataset. For example, V4 with V5 \"only\" has 0.44, but V7 with V8 has 0.86. This means that the dataset has a lot of redundancies.\n",
    "\n",
    "We can use the PCA algorithm from above to reduce the dimensionality of our dataset as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# standardize the data - this is necessary to avoid problems with\n",
    "# different measurement scales!!\n",
    "pcaDataStd = (X-X.mean())/X.std()\n",
    "# construct PCA method\n",
    "bigPCA = PCA()\n",
    "# fit it to data using default parameters\n",
    "bigPCA.fit(pcaDataStd)\n",
    "# how many dimensions do we have \n",
    "numDims = len(bigPCA.explained_variance_)\n",
    "plt.figure()\n",
    "# plot the cumulative sum of the explained variance\n",
    "plt.plot(np.arange(1,numDims+1),np.cumsum(bigPCA.explained_variance_)*100.0/numDims,'ko-')\n",
    "plt.xticks(np.arange(1,numDims+1))\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Percent total variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are multiple ways to decide now how many components to keep. With this plot, we can try to keep as many components so that we can still explain 80% of the variance. In this case, this would be 5 components.\n",
    "\n",
    "We see that three components still explain around 64%, so we can try to plot all wines in this new 3D-coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for our 3D plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# let's get the PCA coordinates of all points\n",
    "# the .transform method takes data and projects\n",
    "# it onto the PCA dimensions\n",
    "# from the resulting projection, we will only\n",
    "# keep the first three dimensions!\n",
    "pcaProj = bigPCA.transform(pcaDataStd)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in np.unique(data.loc[:,\"V1\"]):\n",
    "    # find out index belonging to each label\n",
    "    index=data.loc[:,\"V1\"]==i\n",
    "    # scatter the first three dimensions of each wine-label\n",
    "    ax.scatter(pcaProj[index,0],pcaProj[index,1],pcaProj[index,2],label=i)\n",
    "ax.set_xlabel('PC1: {:.2f}'.format(bigPCA.explained_variance_[0]*100.0/numDims))\n",
    "ax.set_ylabel('PC2: {:.2f}'.format(bigPCA.explained_variance_[1]*100.0/numDims))\n",
    "ax.set_zlabel('PC3: {:.2f}'.format(bigPCA.explained_variance_[2]*100.0/numDims))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And, lo and behold, we can actually see a nice separation of the three wine types in this plot - in fact, most of the separability is done by the first and the second dimension.\n",
    "\n",
    "We can therefore see how exploiting correlations can help to find patterns and perhaps aid in clustering as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY - IRIS flowers\n",
    "\n",
    "Now, let's go to the IRIS dataset and do our analysis here as well.\n",
    "\n",
    "First, we load the IRIS dataset into memory from scikit (actually seaborn has that as well, but we will use scikit's version here) and convert it into a pandas DataFrame for use with seaborn.\n",
    "\n",
    "The scikit IRIS version has several fields:\n",
    "\n",
    "* iris.feature_names: a list of the four features along which Fisher measured the flowers\n",
    "* iris.data: an array containing the values of the four features for each flower\n",
    "* iris.target: an integer-coded array telling us which flower it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# load the IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we want to stick that data into a pandas DataFrame and show the pairplot with seaborn similarly to the WINE example from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what are the names of the features?\n",
    "fnames = iris...\n",
    "\n",
    "# let's convert to pandas DataFrame and name\n",
    "# the columns according to the feature names\n",
    "data = pandas.DataFrame(iris...,columns=...)\n",
    "\n",
    "# the DataFrame should also contain the target values\n",
    "# so, we should add them here in a column named \"Y\"\n",
    "# luckily, extending a dataFrame is easy in pandas:\n",
    "data[\"Y\"] = pandas.DataFrame(...)\n",
    "\n",
    "# show the pairplot of all variables\n",
    "# since seaborn by default plots all NUMERIC columns,\n",
    "# it would plot the \"Y\"-column as well, so we need to\n",
    "# pass the \"vars\" argument to restrict plotting to the\n",
    "# feature columns only\n",
    "sb.pairplot(...,vars=...,hue=...,kind=\"reg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded everything and checked that there are, indeed, some correlations, let's apply PCA to all the data to exploit those correlations. \n",
    "\n",
    "We can simply follow the recipe for the WINE data from above for doing these analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct PCA method\n",
    "irisPCA = ...\n",
    "\n",
    "# standardize our data (only the actual numeric data)\n",
    "dataStd = ...\n",
    "\n",
    "# fit PCA to data \n",
    "irisPCA.fit(...)\n",
    "\n",
    "# now project data onto these dimensions:\n",
    "dataTrans = irisPCA.transform(dataStd)\n",
    "\n",
    "# now convert the result back into DataFrame for the first two dimensions\n",
    "dataToPlot = pandas.DataFrame(...,columns=[\"Dim1\",\"Dim2\"])\n",
    "\n",
    "# add the IRIS flowers\n",
    "dataToPlot[\"Y\"]=data[\"Y\"]\n",
    "\n",
    "# and plot the result using seaborn without regression\n",
    "sb.lmplot(x=\"Dim1\",y=\"Dim2\",data=dataToPlot,hue=\"Y\",fit_reg=False)\n",
    "plt.show()\n",
    "# we want to know how much variance we can explain - this is contained\n",
    "# somewhere in irisPCA...\n",
    "plt.title(\"Two dimensions explain {:.2f}% of variance\".format(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
