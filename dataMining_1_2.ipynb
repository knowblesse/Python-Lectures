{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Encoding of nominal variables to numerical variables\n",
    "Let's create a random set of 100 events that took place in one of three cities. The resulting data is one nominal variables with three entries. If we want to compute things with this, often it is useful to convert that nominal data into a number. \n",
    "\n",
    "Examples of this are one-hot encoding for text processing: \n",
    "* given a text, first you do the unigram of your dictionary and each word in the text is an index into that unigram array - let's say we find $n$ different words\n",
    "* let's say, the current word we are looking at can be found at index $i$ in the unigram\n",
    "* we can view this word in the text therefore as an $n$-dimensional vector, which is zero except for the $i$-th component (the one hot element in the otherwise cold vector!)\n",
    "\n",
    "Why do we use this (rather blown-up) representation?\n",
    "\n",
    "Many machine learning algorithms cannot handle nominal data natively and instead work better with feature vectors. Most trees as we have seen, however, can handle nominal data easily and hence, the one-hot encoding is NOT necessary. \n",
    "\n",
    "Incidentially, a variant of one-hot encodings called **dummy encoding** is also used in statistics to convert nominal (categorical) data into a format that can be handled in an easier way by statistics algorithms. Note, however, that they are **not** quite the same - the difference between these two types of encoding is especially tricky for regression models built on categorical data (we will not look into this here!)\n",
    "\n",
    "https://en.wikiversity.org/wiki/Dummy_variable_(statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a sample of unique values\n",
    "randVal = np.random.randint(low=0,high=3,size=100)\n",
    "randSeries = pandas.Series(randVal)\n",
    "# create a mapping dictionary\n",
    "mapper = {0: 'New York', 1: 'London', 2: 'Zurich'}\n",
    "# convert the number data to text\n",
    "nomvar = randSeries.replace(mapper)\n",
    "\n",
    "# with pandas.get_dummies, we can do the encoding:\n",
    "print(pandas.get_dummies(nomvar))\n",
    "\n",
    "# with sklearn.LabelEncoder, we can do this, too:\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# transform to numeric labels - note, we have to reshape this\n",
    "# array since that is required by the next stage\n",
    "labelEncoded = LabelEncoder().fit_transform(nomvar).reshape(-1,1)\n",
    "# transform '2D' array to binary encoding\n",
    "oneHotEncoded = OneHotEncoder().fit_transform(labelEncoded).toarray()\n",
    "print(oneHotEncoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Encoding of ordinal variables to numerical variables\n",
    "We can repeat the same idea with ordinal variables, but here it will be important to preserve their ordering. Hence, the mapping algorithms from pandas will not do the job properly and we have to generate our own mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a sample of unique values\n",
    "randVal = np.random.randint(low=0,high=3,size=100)\n",
    "randSeries = pandas.Series(randVal)\n",
    "# create a mapping dictionary, replacing values\n",
    "mapper = {1: 'small', 2: 'medium', 0: 'large'}\n",
    "ordvar = randSeries.replace(mapper)\n",
    "# this yields a numeric representation of the \"ordinal\" array\n",
    "pFact,pFactInd = pandas.factorize(ordvar)\n",
    "# if we execute the code multiple times, we will see\n",
    "# that pandas chooses the index randomly\n",
    "# --> order is not preserved!\n",
    "print(pFact,pFactInd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, this does not work. Let's do this ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# So, we basically do the same thing backwards\n",
    "\n",
    "# choose our own mapping algorithm to go from \n",
    "# ordinal data values to numbers\n",
    "preservingMapper = {'large':2 , 'medium': 1, 'small': 0}\n",
    "# and convert array\n",
    "ordvar.replace(preservingMapper)\n",
    "# now we are fine\n",
    "print(ordvar.replace(preservingMapper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Quick view on numerical data\n",
    "The file http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data contains data on concentrations of 13 different chemicals in wines grown in the same region in Italy that are derived from three different cultivations. Hence, for each wine we have 13 different numerical values, like this:\n",
    "\n",
    "`1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065`\n",
    "`1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050`\n",
    "`...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the data from the internets using pandas\n",
    "data = pandas.read_csv((\"http://archive.ics.uci.edu/ml/\"\n",
    "                        \"machine-learning-databases/wine/wine.data\"), header=None)\n",
    "\n",
    "# rename column names to “V1” .. “V14”\n",
    "data.columns=[\"V\"+str(i) for i in range(1,len(data.columns)+1)]\n",
    "\n",
    "# map the numbers to some nicer label\n",
    "labelMapper = {1:'Label1', 2:'Label2', 3:'Label3'}\n",
    "# and convert array (I have to reassign it!)\n",
    "data.V1=data.V1.replace(labelMapper)\n",
    "\n",
    "# print out subview of the data\n",
    "# (note that slicing of pandas DataFrame data directly\n",
    "# is done via the \".loc\" property)\n",
    "print(data.loc[0:10,\"V1\":\"V5\"])\n",
    "\n",
    "# The first column contains the 3 labels \n",
    "# we call this the \"dependent variable\"\n",
    "# Usually, some algorithm would try to \n",
    "# --- characterize the 13 different chemicals for\n",
    "#     each of the 3 labels\n",
    "# --- predict this label from the 13 different \n",
    "#     chemical dimensions in the remaining data\n",
    "y = data.V1\n",
    "\n",
    "# get all the chemical variables for each wine into X\n",
    "X = data.loc[:, \"V2\":]\n",
    "\n",
    "\n",
    "# to drive the point home that .loc indexing of DataFrames\n",
    "# has inclusive indexing:\n",
    "# this give us the first 11 rows\n",
    "numpyArray=np.array(X.loc[0:10,\"V2\":\"V5\"])\n",
    "# nope - we get only the first 10 rows!!\n",
    "print(numpyArray[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now let's take a look at the data using a scatter \n",
    "# matrix plot across the first 4 data dimensions\n",
    "pandas.tools.plotting.scatter_matrix(data.loc[:,\"V2\":\"V5\"],diagonal=\"hist\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Observations\n",
    "* We can see that the histograms (on the diagonal) for some values are decidedly skewed and hence we should be careful about some statistical statements (such as all those having to do with Gaussian/normal assumptions!)\n",
    "* It looks like there are some correlations in that data. For example, V4 versus V5. \n",
    "\n",
    "Given our look at the data, let's plot the correlation for V4 versus V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "# this makes sure that the axes are drawn BELOW the data\n",
    "ax.set_axisbelow(True)\n",
    "plt.grid()\n",
    "# loop through all labels\n",
    "for i in np.unique(data.loc[:,\"V1\"]):\n",
    "    # find out index belonging to each label\n",
    "    index=data.loc[:,\"V1\"]==i\n",
    "    # scatter this\n",
    "    plt.scatter(data.loc[index,\"V4\"],data.loc[index,\"V5\"],label=i)\n",
    "plt.xlabel('V4')\n",
    "plt.ylabel('V5')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Observations\n",
    "* there is a clear correlation between the two chemicals\n",
    "* there may be some discriminative power for the three labels going from the bottom right to the top left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ann = plt.annotate('discriminative\\ndirection?',xy=(2,27.5),xytext=(2.75,12.5),arrowprops=dict(arrowstyle='->'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PCA\n",
    "Could we try to get these directions (the correlation direction and the discriminative direction) from the data? \n",
    "\n",
    "Principal Component Analysis to the rescue. The two eigenvectors of PCA exactly describe these two directions for us.\n",
    "\n",
    "As a reminder: PCA rotates the original data coordinates such that the new axes point in the direction of maximum variances. The new axes are orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# get data\n",
    "pcaData = np.array(X.loc[:,\"V4\":\"V5\"])\n",
    "pcaDataM = pcaData.mean(axis=0)\n",
    "# standardize the data\n",
    "pcaDataStd = (pcaData-pcaDataM)\n",
    "# construct the method\n",
    "pca = PCA()\n",
    "# fit the method to the data\n",
    "pca.fit(pcaDataStd)\n",
    "# now the pca contains the results from\n",
    "# which we want the \"components_\" data\n",
    "vec1 = pca.components_[0]+pcaDataM\n",
    "vec2 = pca.components_[1]+pcaDataM\n",
    "plt.plot([pcaDataM[0],vec1[0]],[pcaDataM[1],vec1[1]],'k')\n",
    "plt.plot([pcaDataM[0],vec2[0]],[pcaDataM[1],vec2[1]],'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a second! These do **not** seem orthogonal! What happened?\n",
    "\n",
    "Let's re-adjust the aspect ratio properly and check that the two components are indeed orthogonal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ann.remove()\n",
    "ax.set_aspect('equal')\n",
    "print('The dot product between the two components (should be 0) is: ',np.dot(pca.components_[0],pca.components_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn\n",
    "Python has an abundance of helper packages for visualization, plotting, data analysis, etc.\n",
    "\n",
    "One particularly nice one is seaborn which is a higher-level interface for matplotlib, allowing you to plot a few things in a \"nicer\" way.\n",
    "\n",
    "You can install it as usual with `pip3 install seaborn` and check some examples here:\n",
    "\n",
    "https://seaborn.pydata.org/examples/index.html\n",
    "\n",
    "To see how things are different compared to matplotlib, let's re-plot the scatter matrix of the wine-data with seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "sb.pairplot(data.loc[:,\"V1\":\"V5\"],hue=\"V1\",kind=\"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What seaborn shows here is:\n",
    "* the cumulative histograms on the diagonal for each of the three labels\n",
    "* the correlation scatter plots including standard, linear regression fits with confidence intervals\n",
    "\n",
    "We can see that V4 versus V5 has the clearest correlations.\n",
    "\n",
    "Let's look at the matrix of correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pandas has a correlation helper function for DataFrames:\n",
    "c = X.corr()\n",
    "# and seaborn has a nice visualization:\n",
    "fig, ax = plt.subplots(figsize=(8,8)) \n",
    "sb.heatmap(c, vmin=-1., vmax=1., square=True, annot=True, fmt='.2f', ax=ax).xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we can see a lot of correlations in our dataset. For example, V4 with V5 \"only\" has 0.44, but V7 with V8 has 0.86. This means that the dataset has a lot of redundancies.\n",
    "\n",
    "We can use the PCA algorithm from above to reduce the dimensionality of our dataset as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is not necessary, but I want to use some numpy-stuff later\n",
    "pcaData = np.array(X)\n",
    "pcaDataM = pcaData.mean(axis=0)\n",
    "pcaDataS = pcaData.std(axis=0)\n",
    "# standardize the data - this is necessary to avoid problems with\n",
    "# different measurement scales!!\n",
    "pcaDataStd = (pcaData-pcaDataM)/pcaDataS\n",
    "# construct PCA method\n",
    "bigPCA = PCA()\n",
    "# fit it to data using default parameters\n",
    "bigPCA.fit(pcaDataStd)\n",
    "# how many dimensions do we have \n",
    "numDims = len(bigPCA.explained_variance_)\n",
    "plt.figure()\n",
    "# plot the cumulative sum of the explained variance\n",
    "plt.plot(np.arange(1,numDims+1),np.cumsum(bigPCA.explained_variance_)*100.0/numDims,'ko-')\n",
    "plt.xticks(np.arange(1,numDims+1))\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Percent total variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to decide now how many components to keep. With this plot, we can try to keep as many components so that we can still explain 80% of the variance. In this case, this would be 5 components.\n",
    "\n",
    "We see that three components still explain around 64%, so we can try to plot all wines in this new 3D-coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for our 3D plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# let's get the PCA coordinates of all points\n",
    "# the .transform method takes data and projects\n",
    "# it onto the PCA dimensions\n",
    "# from the resulting projection, we will only\n",
    "# keep the first three dimensions!\n",
    "pcaProj = bigPCA.transform(pcaDataStd)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in np.unique(data.loc[:,\"V1\"]):\n",
    "    # find out index belonging to each label\n",
    "    index=data.loc[:,\"V1\"]==i\n",
    "    # scatter the first three dimensions of each wine-label\n",
    "    ax.scatter(pcaProj[index,0],pcaProj[index,1],pcaProj[index,2],label=i)\n",
    "ax.set_xlabel('PC1: {:.2f}'.format(bigPCA.explained_variance_[0]*100.0/numDims))\n",
    "ax.set_ylabel('PC2: {:.2f}'.format(bigPCA.explained_variance_[1]*100.0/numDims))\n",
    "ax.set_zlabel('PC3: {:.2f}'.format(bigPCA.explained_variance_[2]*100.0/numDims))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, lo and behold, we can actually see a nice separation of the three wine types in this plot - in fact, most of the separability is done by the first and the second dimension.\n",
    "\n",
    "We can therefore see how exploiting correlations can help to find patterns and perhaps aid in clustering as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
