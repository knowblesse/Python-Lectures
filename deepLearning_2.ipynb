{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation instructions for `nnet`\n",
    "\n",
    "Go to `https://github.com/andersbll/nnet` and download the package.\n",
    "\n",
    "Install Cython\n",
    "\n",
    "Go here:\n",
    "https://github.com/cython/cython/wiki/CythonExtensionsOnWindows\n",
    "\n",
    "and follow instructions for downloading and using the SDK:\n",
    "\n",
    "`Using Windows SDK C/C++ compiler (works for all Python versions)`\n",
    "\n",
    "Make sure to select the correct version of Visual Studio to download!!!\n",
    "\n",
    "\n",
    "\n",
    "Call `python3 setup.py install` and hope for the best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network to play around with\n",
    "\n",
    "http://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_boston, fetch_mldata\n",
    "from sklearn.linear_model import LinearRegression, RidgeClassifierCV\n",
    "import sys\n",
    "import time\n",
    "from skimage import data, io, filters\n",
    "# yes, I know - this is the installation of the github version of the nnet package on my computer\n",
    "# it may and will probably vary on your's\n",
    "sys.path.append(\"/usr/local/lib/python3.5/site-packages/nnet-0.1-py3.5-macosx-10.9-x86_64.egg\")\n",
    "import nnet\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On convolution\n",
    "\n",
    "Before we dive into Convolutional Nets, let's check convolution itself. \n",
    "\n",
    "In the following we investigate a few convolution kernels, or filters, and their effect on a picture when you use them for convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mandrill = io.imread('data/mandrill_male.jpg',as_grey=True)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "io.imshow(mandrill)\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements convolution in the most unoptimized, but clear-to-read way. It cuts out patches of appropriate filter size from a padded image and evaluates the dot product between the patch and the filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to do - painfully slow - convolution\n",
    "def convolve(image, kernel):\n",
    "    # get sizes of image and filter/kernel\n",
    "    (iH, iW) = image.shape\n",
    "    (kH, kW) = kernel.shape\n",
    "    \n",
    "    # this is how much we need to pad the image to be able to \n",
    "    # also process convolution at the edges!\n",
    "    pad = int((kW - 1) / 2)\n",
    "    # we pad with grey, but many other methods are possible\n",
    "    image = np.lib.pad(image, (pad,pad), 'constant', constant_values=(0.5, 0.5))\n",
    "    # this will contain the convolved image\n",
    "    output = np.zeros_like(image)\n",
    "    \n",
    "    # now loop through the image\n",
    "    for y in np.arange(pad, iH + pad):\n",
    "        for x in np.arange(pad, iW + pad):\n",
    "            # cut out the current patch, which conforms in size to the kernel\n",
    "            currentPatch = image[y - pad:y + pad + 1, x - pad:x + pad + 1]\n",
    "            # multiply element-wise and sum\n",
    "            convolvedValues = (currentPatch * kernel).sum()\n",
    "            # write into output picture\n",
    "            output[y - pad, x - pad] = convolvedValues\n",
    "    # return de-padded output picture\n",
    "    return(output[pad:iH,pad:iW])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implement a few different kernels and show their effect on a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# simple normalized blur kernel\n",
    "smallBlur = np.ones((7, 7), dtype=\"float\") * (1.0 / (7 * 7))\n",
    "# bigger blur kernel\n",
    "largeBlur = np.ones((21, 21), dtype=\"float\") * (1.0 / (21 * 21))\n",
    " \n",
    "# sharpening filter\n",
    "sharpen = np.array((\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]), dtype=\"float\")\n",
    "sharpen = sharpen / np.linalg.norm(sharpen)\n",
    "\n",
    "# second derivation\n",
    "laplacian = np.array((\n",
    "    [0, 1, 0],\n",
    "    [1, -4, 1],\n",
    "    [0, 1, 0]), dtype=\"float\")\n",
    "\n",
    "laplacian = laplacian / np.linalg.norm(laplacian)\n",
    " \n",
    "# sobel edge detection (x-axis)\n",
    "sobelX = np.array((\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]), dtype=\"float\")\n",
    "\n",
    "sobelX = sobelX / np.linalg.norm(sobelX)\n",
    " \n",
    "# sobel edge detection (y-axis)\n",
    "sobelY = np.array((\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1]), dtype=\"float\")\n",
    "\n",
    "sobelY = sobelY / np.linalg.norm(sobelY)\n",
    "\n",
    "# gather all filters together in array\n",
    "kernelBank = (\n",
    "    (\"small_blur\", smallBlur),\n",
    "    (\"large_blur\", largeBlur),\n",
    "    (\"sharpen\", sharpen),\n",
    "    (\"laplacian\", laplacian),\n",
    "    (\"sobel_x\", sobelX),\n",
    "    (\"sobel_y\", sobelY)\n",
    ")\n",
    "\n",
    "# loop through each filter, convolve image, and display\n",
    "for (kernelName, kernel) in kernelBank:\n",
    "    print(\"applying {} convolution kernel to image\".format(kernelName))\n",
    "    fix,(ax1, ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "    convolvedImage = convolve(mandrill, kernel)\n",
    "    # we need to renormalize the pictures after convolution\n",
    "    convolvedImage = (convolvedImage - convolvedImage.min())/(convolvedImage.max() - convolvedImage.min())\n",
    "    plt.axes(ax1)\n",
    "    io.imshow(convolvedImage)\n",
    "    ax1.grid('off')\n",
    "    plt.axes(ax2)\n",
    "    io.imshow(convolvedImage[200:400,200:400])\n",
    "    ax2.grid('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Convolutional Neural Network\n",
    "\n",
    "Let's use the most common dataset for this, which is the MNIST handwritten digit dataset. It has been beaten \"to death\" by now with state-of-the-art performance on the pre-defined test set being at roughly 100% by now.\n",
    "\n",
    "This is called \"overfitting by the community\" and even though that sounds like a joke, it actually is a concern!\n",
    "\n",
    "In any case, let's load and prepare the data for our CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch data - caution this is 55MB for the first download!\n",
    "mnist = fetch_mldata('MNIST original', data_home='./data')\n",
    "\n",
    "# split the dataset into train and test and normalize\n",
    "# the first 60000 examples already are the training set\n",
    "split = 60000\n",
    "X_train = np.reshape(mnist.data[:split], (-1, 1, 28, 28))/255.0\n",
    "y_train = mnist.target[:split]\n",
    "# the remaining examples belong to the test set\n",
    "X_test = np.reshape(mnist.data[split:], (-1, 1, 28, 28))/255.0\n",
    "y_test = mnist.target[split:]\n",
    "\n",
    "# for speed purposes do not train on all examples\n",
    "n_train_samples = 20000\n",
    "\n",
    "# this is very important here - we select a random subset!!!\n",
    "# this is done to ensure that the minibatches will actually see\n",
    "# different numbers in each training minibatch!\n",
    "train_idxs = np.random.randint(0, split-1, n_train_samples)\n",
    "X_train = X_train[train_idxs, ...]\n",
    "y_train = y_train[train_idxs, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check with linear classifier\n",
    "\n",
    "Let's use the cross-validated, optimized, multi-class-enabled Ridge Classifier from sklearn to give us some sort of baseline of what a regularized linear classifier can do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcv=RidgeClassifierCV().fit(mnist.data[train_idxs,...]/255.0,mnist.target[train_idxs])\n",
    "rcv.score(mnist.data[split:,...]/255.0,mnist.target[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too bad. Chance performance is 10%, so obviously, we are doing something here. \n",
    "\n",
    "Now let's set up and train a very, very simple convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup one-layer, super-simple convolutional neural network\n",
    "nn = nnet.NeuralNetwork(\n",
    "    layers=[\n",
    "        # first layer is CONV layer with 5x5 and 2 stride-length\n",
    "        # the weight_scale is standard deviation of the Gaussian\n",
    "        # with which weights will be initialized\n",
    "        nnet.Conv(\n",
    "            n_feats=12,\n",
    "            filter_shape=(5, 5),\n",
    "            strides=(2, 2),\n",
    "            weight_scale=0.1,\n",
    "        ),\n",
    "        # we pipe that through a RELU non-linearity\n",
    "        nnet.Activation('relu'),\n",
    "        # and we are already done\n",
    "        nnet.Flatten(),\n",
    "        # make a simple linear perceptron output\n",
    "        nnet.Linear(\n",
    "            n_out=n_classes,\n",
    "            weight_scale=0.1,\n",
    "        ),\n",
    "        # and pipe that through a logistic regression so that \n",
    "        # we can get nice probabilities as interpretations\n",
    "        # notice, that we can use simple logistic regression, \n",
    "        # as the nnet uses a ONEHOT encoding for classes!!!\n",
    "        nnet.LogRegression(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "# the algorithm uses mini-batch stochastic gradient descent\n",
    "# so let's put in a nice chunk of data to upgrade our gradient!\n",
    "nn.fit(X_train, y_train, learning_rate=0.1, max_iter=3, batch_size=256)\n",
    "t1 = time.time()\n",
    "print('Duration: %.1fs' % (t1-t0))\n",
    "\n",
    "# how well does the trained CNN do on test-data?\n",
    "error = nn.error(X_test, y_test)\n",
    "print('Test error rate: %.4f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the loss (and the training error rate) is nicely decreasing. Note, that the gradient is updated after each of the mini-batches until the full training set is exhausted once (i.e., after one epoch). The loss and training error are only given after each epoch for your information, but they change after each mini-batch of course!\n",
    "\n",
    "# Changing mini-batch size\n",
    "\n",
    "Let's see what the effect of that is. Let's make it smaller! This means that the gradient is estimated on a much smaller subset of the data. It also means that the gradient is updated a LOT more per epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "iter 1, loss 0.1648, train error 0.0503\n",
      "iter 2, loss 0.1273, train error 0.0399\n",
      "iter 3, loss 0.1054, train error 0.0340\n",
      "Duration: 178.9s\n",
      "Test error rate: 0.0476\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# the algorithm uses mini-batch stochastic gradient descent\n",
    "# so let's put in a nice chunk of data to upgrade our gradient!\n",
    "nn.fit(X_train, y_train, learning_rate=0.1, max_iter=3, batch_size=20)\n",
    "t1 = time.time()\n",
    "print('Duration: %.1fs' % (t1-t0))\n",
    "\n",
    "# how well does the trained CNN do on test-data?\n",
    "error = nn.error(X_test, y_test)\n",
    "print('Test error rate: %.4f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! That is MUCH better! But why?\n",
    "\n",
    "The reason for this is that the MNIST dataset is actually pretty simple. Hence, even a mere 20 examples can tell you reasonably reliably about the rough gradient direction. Since the gradient is updated 10 times as often with a mini-batch size of 20 compared to 256, we therefore do 10 times as many update steps in the roughly correct direction for EACH epoch. \n",
    "\n",
    "This effect is seen here in the quickly decreasing training error scores.\n",
    "\n",
    "With today's GPUs, a larger mini-batch size is actually computationally better, since the GPU can take full advantage of parallelism in one large gradient update [up to the maximum of GPU memory, of course].\n",
    "\n",
    "If the overall training algorithm (that is the one that does the actual weight updates) is robust, both small and large mini-batch sizes should eventually arrive at the same accuracy! \n",
    "\n",
    "In practice, performance and training times do depend a lot on the dataset and additional factors, such as GPU RAM and bottlenecks in CPU->GPU copying."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
