{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Linear regression - one variable\n",
    "\n",
    "Let's do a simple linear regression on the boston housing data. This includes 13 descriptors for different houses that can be used to try to predict the observed housing price.\n",
    "\n",
    "Let's first load it and take a look at the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "# get correlation matrix:\n",
    "c = np.corrcoef(boston.data.T)\n",
    "# use seaborn to visualize the matrix:\n",
    "fig, ax = plt.subplots(figsize=(8,8)) \n",
    "sb.heatmap(c, vmin=-1., vmax=1., square=True, annot=True, fmt='.2f', ax=ax).xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are a LOT of descriptor correlations here. In addition, we see that the fourth descriptor is very only weakly correlated with anything. Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sb.pairplot(pandas.DataFrame(boston.data[:,0:5],columns=boston.feature_names[0:5]),size=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ouch. Descriptor \"CHAS\" is actually a categorical variable! In addition, we see that other factors (such as \"CRIM\") have very skewed distributions. This is certainly a challenging dataset! \n",
    "\n",
    "So, let's take one of the features and let's try to predict `boston.target`, i.e., the housing prices from it with a linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# one variable from the housing data - since we only have one feature, we need to\n",
    "# reshape the data, so that LinearRegression properly fits!\n",
    "lstat = boston.data[:,-1].reshape(-1,1)\n",
    "# do the fitting \n",
    "lr = LinearRegression().fit(lstat,boston.target)\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(lstat,boston.target)\n",
    "# use predict to conveniently plot the line!\n",
    "plt.plot(lstat,lr.predict(lstat),'k')\n",
    "plt.xlabel('lstat')\n",
    "plt.ylabel('housing price')\n",
    "# equation of the line and explained r^2 in title\n",
    "plt.title('hp = {:.3f} * ls + {:.3f} | R^2={:.3f}'.format(lr.coef_[0],lr.intercept_,lr.score(lstat,boston.target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, we can see that there seems to be a good predictability from \"LSTAT\", that is, the number of lower status households in a neighborhood. We can also see that\n",
    "\n",
    "* there seems to be a cap to housing prices at $50K, providing a potentially problematic ceiling\n",
    "* the relationship between \"LSTAT\" and housing price may actually be non-linear!\n",
    "\n",
    "\n",
    "To address the second point, let's do something very simple. Let's transform our \"LSTAT\" variable logarithmically. A logarithmic transform is useful if the distribution of the variable is skewed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# one variable from the housing data - since we only have one feature, we need to\n",
    "# reshape the data, so that LinearRegression properly fits!\n",
    "lstat = boston.data[:,-1].reshape(-1,1)\n",
    "# do the fitting \n",
    "lr = LinearRegression().fit(np.log(lstat),boston.target)\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(lstat,boston.target)\n",
    "# use predict to conveniently plot the line!\n",
    "plt.scatter(lstat,lr.predict(np.log(lstat)))\n",
    "plt.xlabel('lstat')\n",
    "plt.ylabel('housing price')\n",
    "# equation of the line and explained r^2 in title\n",
    "plt.title('hp = {:.3f} * log(ls) + {:.3f} | R^2={:.3f}'.format(lr.coef_[0],lr.intercept_,lr.score(np.log(lstat),boston.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Aha - that gave us a comfortable boost in $R^2$! Nice!\n",
    "\n",
    "Let's try to see what the log did to our data distribution. To measure the skewedness of the distribution mathematically, we are going to use the skewedness measure (who would have thunk it!):\n",
    "\n",
    "$\\gamma = \\frac{\\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^3}{\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^3}}$\n",
    "\n",
    "This is the third standardized moment of a distribution (with the first two moments being \"0\" and \"1\" by definition, and the fourth being the kurtosis).\n",
    "\n",
    "`numpy` does not have skew as a function, so we are going to resort to `pandas`. We could have also used `scipy` for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,4))\n",
    "fig.add_subplot(121)\n",
    "# plot histogram\n",
    "plt.hist(lstat)\n",
    "# convert to pandas DataFrame, so we have access to skew\n",
    "tmp=pandas.DataFrame(lstat)\n",
    "plt.title('LSTAT: skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "fig.add_subplot(122)\n",
    "# plot histogram for log data\n",
    "plt.hist(np.log(lstat))\n",
    "tmp=pandas.DataFrame(np.log(lstat))\n",
    "plt.title('log(LSTAT): skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, that seemed to have helped a little with the symmetry. \n",
    "\n",
    "But what about the house prices? They may be skewed, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,4))\n",
    "fig.add_subplot(121)\n",
    "plt.hist(boston.target)\n",
    "tmp=pandas.DataFrame(boston.target)\n",
    "plt.title('Prices: skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "fig.add_subplot(122)\n",
    "plt.hist(np.log(boston.target))\n",
    "tmp=pandas.DataFrame(np.log(boston.target))\n",
    "plt.title('log(Prices): skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "They seem to be! So let's do a log-log fit! Let's transform both values logarithmically and then fit a line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# one variable from the housing data - since we only have one feature, we need to\n",
    "# reshape the data, so that LinearRegression properly fits!\n",
    "lstat = boston.data[:,-1].reshape(-1,1)\n",
    "# do the log-log fitting \n",
    "lr = LinearRegression().fit(np.log(lstat),np.log(boston.target))\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(np.log(lstat),np.log(boston.target))\n",
    "# use predict to conveniently plot the line!\n",
    "plt.scatter(np.log(lstat),lr.predict(np.log(lstat)))\n",
    "plt.xlabel('lstat')\n",
    "plt.ylabel('housing price')\n",
    "# equation of the line and explained r^2 in title\n",
    "plt.title('log(hp) = {:.3f} * log(ls) + {:.3f} | R^2={:.3f}'.format(lr.coef_[0],lr.intercept_,lr.score(np.log(lstat),np.log(boston.target))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So that had another (albeit smaller) boost for $R^2$. \n",
    "\n",
    "But is this really a robust increase? Maybe fiddling too much with the data will at some point have adverse effects!\n",
    "\n",
    "We would be able answer this question, if we try to repeat the same things using separate training- and test-sets for the data - see later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DIY: $R^2$ competition\n",
    "\n",
    "Find out which **untransformed** variable has the best explainability in terms of $R^2$-value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# go through all variables\n",
    "\n",
    "    # fit current variable with boston.target\n",
    "\n",
    "    # put r^2-value in a list\n",
    "    \n",
    "# find maximum index of that list and print out winning feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multivariate linear regression\n",
    "\n",
    "Now, let's try to build a multivariate linear model. We can do this in the exact same way in `scikit`, simply by putting in our multidimensional samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make full model\n",
    "lr = LinearRegression().fit(boston.data,boston.target)\n",
    "# plot the coefficients of the linear fit (13 of them)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.bar(range(13),lr.coef_)\n",
    "plt.xticks(range(13),boston.feature_names,rotation='vertical')\n",
    "plt.title('R^2={:.3f}'.format(lr.score(boston.data,boston.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We observe that the $R^2$-value is much higher now (at 0.74), meaning that the additional variables help in increasing the linear predictability of the house prices.\n",
    "\n",
    "But is that really surprising? I mean after all instead of giving the algorithm two features (including the intercept) to predict a value, we are now giving it 14! So we have a **lot** more power. In fact, this power may also cause problems, as we are perhaps doing too much. \n",
    "\n",
    "## Adjusted $R^2$\n",
    "\n",
    "People have therefore come up with a way to correct the $R^2$-value depending on the number of feature dimensions:\n",
    "\n",
    "$R^2_{adj} = R^2 - \\frac{p}{n-p-1}(1-R^2)$ with $p$ being number of feature dimensions and $n$ being the number of samples.\n",
    "\n",
    "In our case, $R^2_{adj} = 0.734$, which is only a little lower since we do have a lot samples ($n>>p$)!\n",
    "\n",
    "## Importance of one feature\n",
    "\n",
    "Also, apparently Nitric Oxides (\"NOX\") have a **huge** negative effect on housing prices. Or do they? \n",
    "\n",
    "## DIY: improve the interpretability of the linear regression\n",
    "\n",
    "We can do this very easily, namely by..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# implement the fix to improve interpretability\n",
    "\n",
    "#\n",
    "#\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.bar(range(13),lr.coef_)\n",
    "plt.xticks(range(13),boston.feature_names,rotation='vertical')\n",
    "plt.title('R^2={:.3f}'.format(lr.score(data,boston.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ridge regression\n",
    "\n",
    "Let's revisit the housing data with ridge regression. The implementation in `scikit` uses the name `alpha` for what we called $\\lambda$ as the influence parameter. We will give this a nice range to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# set range of alphas\n",
    "alphas = np.power(10,np.arange(-5,10,0.25))\n",
    "\n",
    "# array for holding the r^2 values\n",
    "r2s = np.zeros_like(alphas)\n",
    "\n",
    "coefs = []\n",
    "\n",
    "# go through all alphas\n",
    "for idx,alpha in enumerate(alphas):\n",
    "    # do the ridge regression and record r^2\n",
    "    rr = Ridge(alpha = alpha).fit(data,target)\n",
    "    r2s[idx]=rr.score(data,target)\n",
    "    coefs.append(rr.coef_)\n",
    "    \n",
    "# plot the r^2-values of the ridge regression fit\n",
    "fig, ax = plt.subplots(2,1,figsize=(8,4))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.arange(-5,10,0.25),r2s)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.arange(-5,10,0.25),coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Well, as we can see, we can see nothing. The housing data does not benefit whatsoever from doing the ridge regression. The $R^2$-values decrease as we increase `alpha`.\n",
    "\n",
    "That is weird.\n",
    "\n",
    "## Evaluating model quality properly\n",
    "\n",
    "Let's try to get back to the idea of splitting our data into training and test sets. If we only use part of the data for training, and another part for testing, we can use several of such splits to evaluate how good the algorithms can **generalize** from training to testing. \n",
    "\n",
    "So, in the following, we train our linear regression and ridge regression models in this way. For each `alpha`-value, we run multiple splits of our big dataset into smaller datasets. We train the regressors on the training set and evaluate their performance on the test set, averaging over the multiple splits in each case.\n",
    "\n",
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def testBostonRidge(data,target,numtrain=250,numShuffle = 20, mode = 'r2'):\n",
    "    # set range of alphas\n",
    "    exps = np.arange(-5,5,0.25)\n",
    "    alphas = np.power(10,exps)\n",
    "\n",
    "    # array for holding the r^2 values\n",
    "    r2s_train = np.zeros((numShuffle,alphas.size))\n",
    "    r2s_test = np.zeros((numShuffle,alphas.size))\n",
    "    lr2s_train = np.zeros((numShuffle,alphas.size))\n",
    "    lr2s_test = np.zeros((numShuffle,alphas.size))\n",
    "\n",
    "\n",
    "    for it in range(0,numShuffle):\n",
    "        randIdx = np.arange(0,target.size)\n",
    "        np.random.shuffle(randIdx)\n",
    "        trainIdx = randIdx[:numTrain]\n",
    "        testIdx = randIdx[numTrain:]\n",
    "        # go through all alphas\n",
    "        for idx,alpha in enumerate(alphas):\n",
    "            if mode=='r2':\n",
    "                # do the ridge regression and record r^2\n",
    "                # ATTENTION: r^2 can and will be negative sometimes!!\n",
    "                rr = Ridge(alpha = alpha).fit(data[trainIdx,:],target[trainIdx])\n",
    "                r2s_train[it,idx]=rr.score(data[trainIdx,:],target[trainIdx])\n",
    "                r2s_test[it,idx]=rr.score(data[testIdx,:],target[testIdx])\n",
    "                lr = LinearRegression().fit(data[trainIdx,:],target[trainIdx])\n",
    "                lr2s_train[it,idx]=lr.score(data[trainIdx,:],target[trainIdx])\n",
    "                lr2s_test[it,idx]=lr.score(data[testIdx,:],target[testIdx])\n",
    "            else:\n",
    "                # do the ridge regression and record mse\n",
    "                # NOTE: MSE is always positive, but a less good measure of generalization\n",
    "                rr = Ridge(alpha = alpha).fit(data[trainIdx,:],target[trainIdx])\n",
    "                r2s_train[it,idx]=np.sum((rr.predict(data[trainIdx,:])-target[trainIdx])**2)/len(trainIdx)\n",
    "                r2s_test[it,idx]=np.sum((rr.predict(data[testIdx,:])-target[testIdx])**2)/len(testIdx)\n",
    "                lr = LinearRegression().fit(data[trainIdx,:],target[trainIdx])\n",
    "                lr2s_train[it,idx]=np.sum((lr.predict(data[trainIdx,:])-target[trainIdx])**2)/len(trainIdx)\n",
    "                lr2s_test[it,idx]=np.sum((lr.predict(data[testIdx,:])-target[testIdx])**2)/len(testIdx)\n",
    "\n",
    "    # plot the r^2-values of the ridge regression fit\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    plt.plot(exps,np.median(r2s_train,axis=0),label='rrTrain')\n",
    "    plt.plot(exps,np.median(r2s_test,axis=0),label='rrTest')\n",
    "    plt.plot(exps,np.median(lr2s_train,axis=0),label='lrTrain')\n",
    "    plt.plot(exps,np.median(lr2s_test,axis=0),label='lrTest')\n",
    "    plt.legend(loc='best')\n",
    "    if mode == 'r2':\n",
    "        plt.title('found {:d} alpha-values that are better for Ridge in testing'.format(np.size(np.where(np.median(r2s_test,axis=0)>np.median(lr2s_test,axis=0)))))\n",
    "    else:\n",
    "        plt.title('found {:d} alpha-values that are better for Ridge in testing'.format(np.size(np.where(np.median(r2s_test,axis=0)<np.median(lr2s_test,axis=0)))))\n",
    "\n",
    "boston = load_boston()    \n",
    "data = boston.data\n",
    "target = boston.target\n",
    "# normalization is necessary for ridge regression, since the \n",
    "# weight penalties are NOT scale-invariant\n",
    "data = (data-data.mean(axis=0))/data.std(axis=0)\n",
    "testBostonRidge(data,target,450,100,'r2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There we go!\n",
    "\n",
    "Now the ridge regression beats out linear regression **on the test set** for certain values of alpha. \n",
    "\n",
    "In addition, we see clearly that performance on the test set is **worse** than on the training set. This is the problem of generalization!\n",
    "\n",
    "By changing the code, we can also see that more training data makes the two algorithms more and more similar. In addition, training and testing performance pull together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More feature engineering\n",
    "\n",
    "Now let's go for non-linear features. One easy way to make a new feature is to simply calculate a non-linear function of an existing feature and add it to your data. Of course, this is not without danger, since you could easily overfit, but for now, we can try to see where we get.\n",
    "\n",
    "For the beginning, let's do multiplicative pairwise combinations of all 13 housing features we have in the data. This will be (13 choose 2) = 91 different combinations that we can make. So we add them to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bostonExt = load_boston()\n",
    "for i in range(13):\n",
    "    for j in range(i,13):\n",
    "        newCol = boston.data[:,i]*boston.data[:,j]\n",
    "        bostonExt.data = np.c_[bostonExt.data,newCol]\n",
    "print(np.shape(bostonExt.data))\n",
    "\n",
    "# normalization is necessary for ridge regression, since the \n",
    "# weight penalties are NOT scale-invariant\n",
    "bostonExt.data = (bostonExt.data-bostonExt.data.mean(axis=0))/bostonExt.data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's test our new dataset. Since we now have over 100 features, we better take care to train on more data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testBostonRidge(bostonExt.data,bostonExt.target,350,100,'r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LASSO regression\n",
    "\n",
    "Now let's try to use LASSO regression, which optimizes an L1-penalized cost function:\n",
    "\n",
    "$\\sum (x_i - \\sum w_j a_j)^2 + \\lambda \\sum |w_i|$\n",
    "\n",
    "This cost function is supposed to lead to coefficients that are zero. Let's first use it on the original housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def testBostonLASSO(data,target,numtrain=250,numShuffle = 20, mode = 'r2'):\n",
    "    # set range of alphas\n",
    "    exps = np.arange(-5,5,0.25)\n",
    "    alphas = np.power(10,exps)\n",
    "\n",
    "    # array for holding the r^2 values\n",
    "    lars2s_train = np.zeros((numShuffle,alphas.size))\n",
    "    lars2s_test = np.zeros((numShuffle,alphas.size))\n",
    "    r2s_train = np.zeros((numShuffle,alphas.size))\n",
    "    r2s_test = np.zeros((numShuffle,alphas.size))\n",
    "    lr2s_train = np.zeros((numShuffle,alphas.size))\n",
    "    lr2s_test = np.zeros((numShuffle,alphas.size))\n",
    "\n",
    "\n",
    "    for it in range(0,numShuffle):\n",
    "        randIdx = np.arange(0,target.size)\n",
    "        np.random.shuffle(randIdx)\n",
    "        trainIdx = randIdx[:numTrain]\n",
    "        testIdx = randIdx[numTrain:]\n",
    "        # go through all alphas\n",
    "        for idx,alpha in enumerate(alphas):\n",
    "            if mode=='r2':\n",
    "                # do the ridge regression and record r^2\n",
    "                # ATTENTION: r^2 can and will be negative sometimes!!\n",
    "                lars = Lasso(alpha = alpha).fit(data[trainIdx,:],target[trainIdx])\n",
    "                lars2s_train[it,idx]=lars.score(data[trainIdx,:],target[trainIdx])\n",
    "                lars2s_test[it,idx]=lars.score(data[testIdx,:],target[testIdx])\n",
    "\n",
    "                lr = LinearRegression().fit(data[trainIdx,:],target[trainIdx])\n",
    "                lr2s_train[it,idx]=lr.score(data[trainIdx,:],target[trainIdx])\n",
    "                lr2s_test[it,idx]=lr.score(data[testIdx,:],target[testIdx])\n",
    "            else:\n",
    "                # do the ridge regression and record mse\n",
    "                # NOTE: MSE is always positive, but a less good measure of generalization\n",
    "                lars = Lasso(alpha = alpha).fit(data[trainIdx,:],target[trainIdx])\n",
    "                lars2s_train[it,idx]=np.sum((lars.predict(data[trainIdx,:])-target[trainIdx])**2)/len(trainIdx)\n",
    "                lars2s_test[it,idx]=np.sum((lars.predict(data[testIdx,:])-target[testIdx])**2)/len(testIdx)\n",
    "\n",
    "                lr = LinearRegression().fit(data[trainIdx,:],target[trainIdx])\n",
    "                lr2s_train[it,idx]=np.sum((lr.predict(data[trainIdx,:])-target[trainIdx])**2)/len(trainIdx)\n",
    "                lr2s_test[it,idx]=np.sum((lr.predict(data[testIdx,:])-target[testIdx])**2)/len(testIdx)\n",
    "\n",
    "    # plot the r^2-values of the ridge regression fit\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    plt.plot(exps,np.median(lars2s_train,axis=0),label='larsTrain')\n",
    "    plt.plot(exps,np.median(lars2s_test,axis=0),label='larsTest')\n",
    "    plt.plot(exps,np.median(lr2s_train,axis=0),label='lrTrain')\n",
    "    plt.plot(exps,np.median(lr2s_test,axis=0),label='lrTest')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "boston = load_boston()    \n",
    "data = boston.data\n",
    "target = boston.target\n",
    "# normalization is necessary for ridge regression, since the \n",
    "# weight penalties are NOT scale-invariant\n",
    "data = (data-data.mean(axis=0))/data.std(axis=0)\n",
    "testBostonLASSO(data,target,450,100,'r2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see, depending on the number of training data and repetitions, we can get a few higher $R^2$ values.\n",
    "\n",
    "Let's look at the coefficients $w_i$ for the original and extended housing datasets for standard linear regression, ridge regression, and LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def loadData(ext=False):\n",
    "    d = load_boston()\n",
    "    if (ext):       \n",
    "        for i in range(13):\n",
    "            for j in range(i,13):\n",
    "                newCol = d.data[:,i]*d.data[:,j]\n",
    "                d.data = np.c_[d.data,newCol]\n",
    "                tmp = d.feature_names[i]+d.feature_names[j]\n",
    "                d.feature_names=np.append(d.feature_names,tmp)\n",
    "        print(np.shape(d.data))\n",
    "\n",
    "    # normalization is necessary for ridge regression, since the \n",
    "    # weight penalties are NOT scale-invariant\n",
    "    d.data = (d.data-d.data.mean(axis=0))/d.data.std(axis=0)\n",
    "    return d\n",
    "\n",
    "da = loadData(True)\n",
    "\n",
    "la = Lasso(alpha = 0.5).fit(da.data,da.target)\n",
    "lr = LinearRegression().fit(da.data,da.target)\n",
    "rr = Ridge(alpha = 0.5).fit(da.data,da.target)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "plt.plot(la.coef_,label='LASSO {:.3f}'.format(la.score(da.data,da.target)))\n",
    "plt.plot(rr.coef_,label='Ridge {:.3f}'.format(rr.score(da.data,da.target)))\n",
    "plt.plot(lr.coef_,label='LLS {:.3f}'.format(lr.score(da.data,da.target)))\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(da.feature_names[np.abs(la.coef_)>0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see, for the small dataset there are no big differences. By increasing alpha, we can force ridge to decrease the coefficients compared to linear regression, and we can force LASSO to select only a few features.\n",
    "\n",
    "For the large housing dataset, only a few features (combinations of features) are selected by LASSO. \n",
    "\n",
    "Note, that in all cases the additional penalties come at the \"price\" of decreasing $R^2$ on the data. However, it is important to realize that we **didn't do validation on separate training and testing sets** to get a better estimate of the model generalizability.\n",
    "\n",
    "Let's take a look at this in more detail now:\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "\n",
    "`sklearn` has several helper functions that can be used to implement (nested) cross-validation and to select holdout datasets. In addition, it has helper functions that allow you to do grid searches for several parameters.\n",
    "\n",
    "The following script is adapted from the `sklearn` example page and compares nested and non-nested cross-validation errors for some datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "# this code imports some necessary stuff for displaying the progress bar\n",
    "# in order to view the progress bar, jupyter needs to enable the javascript\n",
    "# you can enable this in the command line with the following command:\n",
    "#\n",
    "# jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "#\n",
    "# after this, you will have to restart the jupyter notebook!\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "# number of repetitions of hold-out\n",
    "numReps = 30\n",
    "\n",
    "# load boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "    \n",
    "X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "\n",
    "# set up possible values of parameters to optimize over\n",
    "# this is a dictionary of parameter names and a list of\n",
    "# parameter values\n",
    "pGrid = {\"alpha\": [0.001, 0.01, 0.05, 0.1, 1, 2, 5, 10]}\n",
    "\n",
    "# initialize the ridge regressor for later\n",
    "rr = Ridge()\n",
    "\n",
    "# here we store the two types of R^2 values for nested\n",
    "# and non-nested cross-validation\n",
    "nonNestedScores = np.zeros(numReps)\n",
    "nestedScores = np.zeros(numReps)\n",
    "\n",
    "# init progress bar\n",
    "f = FloatProgress(min=0, max=numReps)\n",
    "display(f)\n",
    "    \n",
    "# loop for each repetition\n",
    "for i in range(numReps):\n",
    "\n",
    "    # let's select the cross-validation technique for the inner\n",
    "    # and the outer loops - here we do KFold with 10 splits\n",
    "    # we could do \"StratifiedKFold\" (stratification), \n",
    "    #             \"LeaveOneOut\" (more costly)\n",
    "    \n",
    "    # the KFold uses shuffling as well and we pass standard random states\n",
    "    # so the same data folds will be selected if you execute the code\n",
    "    # several times!\n",
    "    innerCV = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    outerCV = KFold(n_splits=10, shuffle=True, random_state=i)\n",
    "\n",
    "    # here we use the inner cross-validation loop to implement\n",
    "    # a grid search over all parameters\n",
    "    clf = GridSearchCV(estimator=rr, param_grid=pGrid, cv=innerCV)\n",
    "    # fit the classifier to the data using cross-validation\n",
    "    clf.fit(X, y)\n",
    "    # select the best model for all parameters from the list and store\n",
    "    nonNestedScores[i] = clf.best_score_\n",
    "    # now run the outer loop in which we split again to average the\n",
    "    # optimally-selected model from the inner loop across the outer\n",
    "    # cross-validation splits:\n",
    "    nestedScore = cross_val_score(clf, X=X, y=y, cv=outerCV)\n",
    "    # the resulting mean is a better estimate of the generalizability \n",
    "    # of the optimally-selected model:\n",
    "    nestedScores[i] = nestedScore.mean()\n",
    "    \n",
    "    # update progress bar\n",
    "    f.value+=1\n",
    "\n",
    "# what is the difference between the two cross-validation loops?\n",
    "scoreDifference = nonNestedScores - nestedScores\n",
    "\n",
    "print(\"Average difference of {0:6f} with std. dev. of {1:6f}.\"\n",
    "      .format(scoreDifference.mean(), scoreDifference.std()))\n",
    "\n",
    "# plot scores of nested and non-nested cross-validation in a violin plot\n",
    "# if zero is in the interval, this means that they do not give vastly\n",
    "# different results\n",
    "plt.figure()\n",
    "sb.violinplot(scoreDifference,orient=\"v\")\n",
    "plt.ylabel('Score Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see, the score distribution is slightly above 0, meaning that the best score selected in the inner loop is a too opimistic estimate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
