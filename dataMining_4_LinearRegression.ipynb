{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression - one variable\n",
    "\n",
    "Let's do a simple linear regression on the boston housing data. This includes 13 descriptors for different houses that can be used to try to predict the observed housing price.\n",
    "\n",
    "Let's first load it and take a look at the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "# get correlation matrix:\n",
    "c = np.corrcoef(boston.data.T)\n",
    "# use seaborn to visualize the matrix:\n",
    "fig, ax = plt.subplots(figsize=(8,8)) \n",
    "sb.heatmap(c, vmin=-1., vmax=1., square=True, annot=True, fmt='.2f', ax=ax).xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a LOT of descriptor correlations here. In addition, we see that the fourth descriptor is very only weakly correlated with anything. Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.pairplot(pandas.DataFrame(boston.data[:,0:5],columns=boston.feature_names[0:5]),size=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch. Descriptor \"CHAS\" is actually a categorical variable! In addition, we see that other factors (such as \"CRIM\") have very skewed distributions. This is certainly a challenging dataset! \n",
    "\n",
    "So, let's take one of the features and let's try to predict `boston.target`, i.e., the housing prices from it with a linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one variable from the housing data - since we only have one feature, we need to\n",
    "# reshape the data, so that LinearRegression properly fits!\n",
    "lstat = boston.data[:,-1].reshape(-1,1)\n",
    "# do the fitting \n",
    "lr = LinearRegression().fit(lstat,boston.target)\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(lstat,boston.target)\n",
    "# use predict to conveniently plot the line!\n",
    "plt.plot(lstat,lr.predict(lstat),'k')\n",
    "plt.xlabel('lstat')\n",
    "plt.ylabel('housing price')\n",
    "# equation of the line and explained r^2 in title\n",
    "plt.title('hp = {:.3f} * ls + {:.3f} | R^2={:.3f}'.format(lr.coef_[0],lr.intercept_,lr.score(lstat,boston.target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that there seems to be a good predictability from \"LSTAT\", that is, the number of lower status households in a neighborhood. We can also see that\n",
    "\n",
    "* there seems to be a cap to housing prices at $50K, providing a potentially problematic ceiling\n",
    "* the relationship between \"LSTAT\" and housing price may actually be non-linear!\n",
    "\n",
    "\n",
    "To address the second point, let's do something very simple. Let's transform our \"LSTAT\" variable logarithmically. A logarithmic transform is useful if the distribution of the variable is skewed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one variable from the housing data - since we only have one feature, we need to\n",
    "# reshape the data, so that LinearRegression properly fits!\n",
    "lstat = boston.data[:,-1].reshape(-1,1)\n",
    "# do the fitting \n",
    "lr = LinearRegression().fit(np.log(lstat),boston.target)\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(lstat,boston.target)\n",
    "# use predict to conveniently plot the line!\n",
    "plt.scatter(lstat,lr.predict(np.log(lstat)))\n",
    "plt.xlabel('lstat')\n",
    "plt.ylabel('housing price')\n",
    "# equation of the line and explained r^2 in title\n",
    "plt.title('hp = {:.3f} * log(ls) + {:.3f} | R^2={:.3f}'.format(lr.coef_[0],lr.intercept_,lr.score(np.log(lstat),boston.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha - that gave us a comfortable boost in $R^2$! Nice!\n",
    "\n",
    "Let's try to see what the log did to our data distribution. To measure the skewedness of the distribution mathematically, we are going to use the skewedness measure (who would have thunk it!):\n",
    "\n",
    "$\\gamma = \\frac{\\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^3}{\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^3}}$\n",
    "\n",
    "This is the third standardized moment of a distribution (with the first two moments being \"0\" and \"1\" by definition, and the fourth being the kurtosis).\n",
    "\n",
    "`numpy` does not have skew as a function, so we are going to resort to `pandas`. We could have also used `scipy` for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,4))\n",
    "fig.add_subplot(121)\n",
    "# plot histogram\n",
    "plt.hist(lstat)\n",
    "# convert to pandas DataFrame, so we have access to skew\n",
    "tmp=pandas.DataFrame(lstat)\n",
    "plt.title('LSTAT: skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "fig.add_subplot(122)\n",
    "# plot histogram for log data\n",
    "plt.hist(np.log(lstat))\n",
    "tmp=pandas.DataFrame(np.log(lstat))\n",
    "plt.title('log(LSTAT): skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that seemed to have helped a little with the symmetry. \n",
    "\n",
    "But what about the house prices? They may be skewed, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,4))\n",
    "fig.add_subplot(121)\n",
    "plt.hist(boston.target)\n",
    "tmp=pandas.DataFrame(boston.target)\n",
    "plt.title('Prices: skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "fig.add_subplot(122)\n",
    "plt.hist(np.log(boston.target))\n",
    "tmp=pandas.DataFrame(np.log(boston.target))\n",
    "plt.title('log(Prices): skew = {:.3f}'.format(tmp.skew()[0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to be! So let's do a log-log fit! Let's transform both values logarithmically and then fit a line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one variable from the housing data - since we only have one feature, we need to\n",
    "# reshape the data, so that LinearRegression properly fits!\n",
    "lstat = boston.data[:,-1].reshape(-1,1)\n",
    "# do the log-log fitting \n",
    "lr = LinearRegression().fit(np.log(lstat),np.log(boston.target))\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(np.log(lstat),np.log(boston.target))\n",
    "# use predict to conveniently plot the line!\n",
    "plt.scatter(np.log(lstat),lr.predict(np.log(lstat)))\n",
    "plt.xlabel('lstat')\n",
    "plt.ylabel('housing price')\n",
    "# equation of the line and explained r^2 in title\n",
    "plt.title('log(hp) = {:.3f} * log(ls) + {:.3f} | R^2={:.3f}'.format(lr.coef_[0],lr.intercept_,lr.score(np.log(lstat),np.log(boston.target))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that had another (albeit smaller) boost for $R^2$. \n",
    "\n",
    "But is this really a robust increase? Maybe fiddling too much with the data will at some point have adverse effects!\n",
    "\n",
    "We would be able answer this question, if we try to repeat the same things using separate training- and test-sets for the data - see later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY: $R^2$ competition\n",
    "\n",
    "Find out which **untransformed** variable has the best explainability in terms of $R^2$-value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go through all variables\n",
    "\n",
    "    # fit current variable with boston.target\n",
    "\n",
    "    # put r^2-value in a list\n",
    "    \n",
    "# find maximum index of that list and print out winning feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate linear regression\n",
    "\n",
    "Now, let's try to build a multivariate linear model. We can do this in the exact same way in `scikit`, simply by putting in our multidimensional samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make full model\n",
    "lr = LinearRegression().fit(boston.data,boston.target)\n",
    "# plot the coefficients of the linear fit (13 of them)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.bar(range(13),lr.coef_)\n",
    "plt.xticks(range(13),boston.feature_names,rotation='vertical')\n",
    "plt.title('R^2={:.3f}'.format(lr.score(boston.data,boston.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the $R^2$-value is much higher now (at 0.74), meaning that the additional variables help in increasing the linear predictability of the house prices.\n",
    "\n",
    "But is that really surprising? I mean after all instead of giving the algorithm two features (including the intercept) to predict a value, we are now giving it 14! So we have a **lot** more power. In fact, this power may also cause problems, as we are perhaps doing too much. \n",
    "\n",
    "## Adjusted $R^2$\n",
    "\n",
    "People have therefore come up with a way to correct the $R^2$-value depending on the number of feature dimensions:\n",
    "\n",
    "$R^2_{adj} = R^2 - \\frac{p}{n-p-1}(1-R^2)$ with $p$ being number of feature dimensions and $n$ being the number of samples.\n",
    "\n",
    "In our case, $R^2_{adj} = 0.734$\n",
    "\n",
    "Also, apparently Nitric Oxides (\"NOX\") have a **huge** negative effect on housing prices. Or do they? \n",
    "\n",
    "## DIY: improve the interpretability of the linear regression\n",
    "\n",
    "We can do this very easily, namely by..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implement the fix to improve interpretability\n",
    "\n",
    "#\n",
    "#\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.bar(range(13),lr.coef_)\n",
    "plt.xticks(range(13),boston.feature_names,rotation='vertical')\n",
    "plt.title('R^2={:.3f}'.format(lr.score(data,boston.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression\n",
    "\n",
    "Let's revisit the housing data with ridge regression. The implementation in `scikit` uses the name `alpha` for what we called $\\lambda$ as the influence parameter. We will give this a nice range to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# set range of alphas\n",
    "alphas = np.power(10,np.arange(-5,10,0.25))\n",
    "\n",
    "# array for holding the r^2 values\n",
    "r2s = np.zeros_like(alphas)\n",
    "\n",
    "coefs = []\n",
    "\n",
    "# go through all alphas\n",
    "for idx,alpha in enumerate(alphas):\n",
    "    # do the ridge regression and record r^2\n",
    "    rr = Ridge(alpha = alpha).fit(data,target)\n",
    "    r2s[idx]=rr.score(data,target)\n",
    "    coefs.append(rr.coef_)\n",
    "    \n",
    "# plot the r^2-values of the ridge regression fit\n",
    "fig, ax = plt.subplots(2,1,figsize=(8,4))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.arange(-5,10,0.25),r2s)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.arange(-5,10,0.25),coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as we can see, we can see nothing. The housing data does not benefit whatsoever from doing the ridge regression. The $R^2$-values decrease as we increase `alpha`.\n",
    "\n",
    "That is weird.\n",
    "\n",
    "Let's try to get back to the idea of splitting our data into training and test sets. If we only use part of the data for training, and another part for testing, we can use several of such splits to evaluate how good the algorithms can **generalize** from training to testing. \n",
    "\n",
    "So, in the following, we train our linear regression and ridge regression models in this way. For each `alpha`-value, we run multiple splits of our big dataset into smaller datasets. We train the regressors on the training set and evaluate their performance on the test set, averaging over the multiple splits in each case.\n",
    "\n",
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# set range of alphas\n",
    "exps = np.arange(-5,10,0.25)\n",
    "alphas = np.power(10,exps)\n",
    "\n",
    "#number of train points\n",
    "numTrain = 50\n",
    "\n",
    "# number of reshufflings per alpha value\n",
    "numShuffle = 20\n",
    "\n",
    "# array for holding the r^2 values\n",
    "r2s_train = np.zeros((numShuffle,alphas.size))\n",
    "r2s_test = np.zeros((numShuffle,alphas.size))\n",
    "lr2s_train = np.zeros((numShuffle,alphas.size))\n",
    "lr2s_test = np.zeros((numShuffle,alphas.size))\n",
    "\n",
    "\n",
    "for it in range(0,numShuffle):\n",
    "    randIdx = np.arange(0,target.size)\n",
    "    np.random.shuffle(randIdx)\n",
    "    trainIdx = randIdx[:numTrain]\n",
    "    testIdx = randIdx[numTrain:]\n",
    "    # go through all alphas\n",
    "    for idx,alpha in enumerate(alphas):\n",
    "        # do the ridge regression and record r^2\n",
    "        rr = Ridge(alpha = alpha).fit(data[trainIdx],target[trainIdx])\n",
    "        r2s_train[it,idx]=rr.score(data[trainIdx],target[trainIdx])\n",
    "        r2s_test[it,idx]=rr.score(data[testIdx],target[testIdx])\n",
    "        lr = LinearRegression().fit(data[trainIdx],target[trainIdx])\n",
    "        lr2s_train[it,idx]=lr.score(data[trainIdx],target[trainIdx])\n",
    "        lr2s_test[it,idx]=lr.score(data[testIdx],target[testIdx])\n",
    "    \n",
    "# plot the r^2-values of the ridge regression fit\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.plot(exps,r2s_train.mean(axis=0),label='rrTrain')\n",
    "plt.plot(exps,r2s_test.mean(axis=0),label='rrTest')\n",
    "plt.plot(exps,lr2s_train.mean(axis=0),label='lrTrain')\n",
    "plt.plot(exps,lr2s_test.mean(axis=0),label='lrTest')\n",
    "plt.legend(loc='best')\n",
    "plt.title('found {:d} alpha-values that are better for Ridge in testing'.format(np.size(np.where(r2s_test.mean(axis=0)>lr2s_test.mean(axis=0)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go!\n",
    "\n",
    "Now the ridge regression beats out linear regression **on the test set** for certain values of alpha. \n",
    "\n",
    "In addition, we see clearly that performance on the test set is **worse** than on the training set. This is the problem of generalization!\n",
    "\n",
    "By changing the code, we can also see that more training data makes the two algorithms more and more similar. In addition, training and testing performance pull together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
