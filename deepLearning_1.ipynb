{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple two-layer, non-linear perceptron\n",
    "\n",
    "We are going to implement a simple, perceptron with a non-linear sigmoid activation function and train it using backpropagation.\n",
    "\n",
    "Weights are initialized with random numbers of mean 0 and then update iteratively using the error between network output and wanted labels. \n",
    "\n",
    "Initially, we are going to test very simple input-output relationships with the input consisting of a 3-dimensional vector with either 0 or 1, and the output consisting of a 1-dimensional target-value also with either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a non-linear activation function\n",
    "# here, we use a sigmoid shape\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# the derivative of the sigmoid\n",
    "def dsigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "# implements a simple two-layer network\n",
    "def twoLayer(X,y,plotting=True):\n",
    "    # seed random numbers to get repeatable results\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # initialize weights randomly (mean 0)\n",
    "    # the number of weights is determined by the number\n",
    "    # of columns in both the input data X and the output\n",
    "    # data y!\n",
    "    syn0 = 2*np.random.random((X.shape[1],y.shape[1])) - 1\n",
    "\n",
    "    # maximum iteration\n",
    "    maxIter = 10000\n",
    "\n",
    "    # store errors\n",
    "    l1ErrorArray = np.zeros(maxIter)\n",
    "\n",
    "    # do some iterations\n",
    "    for it in np.arange(maxIter):\n",
    "\n",
    "        # forward propagation: we put in our pattern\n",
    "        # as layer \"0\" and then push it through the\n",
    "        # activation function to get the output of\n",
    "        # the layer\n",
    "        l0 = X\n",
    "        l1 = sigmoid(np.dot(l0,syn0))\n",
    "\n",
    "        # evaluate the error of the layer\n",
    "        l1Error = y - l1\n",
    "\n",
    "        # evaluate the summed squared error\n",
    "        l1ErrorArray[it] = np.sum(l1Error*l1Error)\n",
    "\n",
    "        # print out the summed squared error sometimes\n",
    "        if (it%1000==0):\n",
    "            sys.stdout.write(\"Iteration {:d}: error = {:f}\\r\".format(it,l1ErrorArray[it]))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # the error determines the amount we need\n",
    "        # to move along the derivative\n",
    "        l1Delta = l1Error * dsigmoid(l1)\n",
    "\n",
    "        # the weight update is the dot product between\n",
    "        # the pattern input and the correction amount \n",
    "        syn0 += np.dot(l0.T,l1Delta)\n",
    "\n",
    "    print(\"output after training is:\\n\",l1)\n",
    "    if (plotting):\n",
    "        fig,ax = plt.subplots(figsize=(8,6))\n",
    "        plt.plot(l1ErrorArray)\n",
    "    return(syn0,l1ErrorArray)\n",
    "    \n",
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1] ])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# note, that each row here corresponds to the\n",
    "# row in X - so we are trying to basically\n",
    "# learn two class labels from data here\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# let's call our function and do the training\n",
    "(weights,errors)=twoLayer(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That went pretty well. We achieved fast learning. \n",
    "\n",
    "Notice, however, that the first row of the training examples was fully correlated to the target values! Hence, among all the training examples, this should have been easy to find out!\n",
    "\n",
    "In fact, let's check the weights of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictably, the first data dimension was given a large positive weight, whereas the other dimensions were given small and/or negative weights.\n",
    "\n",
    "Note, however, that the weights are not the weights that would be given by a linear perceptron, since the weights here sit inside the sigmoid activation function!\n",
    "\n",
    "$Out_j = 1/(1+e^{\\sum_k w_jk x_k})$\n",
    "\n",
    "So, what about a different, less correlated target value vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1] ])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# note, that each row here corresponds to the\n",
    "# row in X - so we are trying to basically\n",
    "# learn two class labels from data here\n",
    "y = np.array([[1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# let's call our function and do the training\n",
    "(weights,errors)=twoLayer(X,y)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works as well. \n",
    "\n",
    "We have seen that networks can learn simple logical functions, so this one should do that too, right? Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,0],\n",
    "                [1,1,0],\n",
    "                [0,1,0],\n",
    "                [0,0,0]])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# here is a simple, logical combination:\n",
    "y = np.logical_or(X[:,0],np.logical_and(X[:,1],X[:,2]))\n",
    "print(y)\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors)=twoLayer(X,y.reshape(-1,1))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we forgot something! Right now, what we can do is to change the weights on the sigmoids. But we have no way to shift the whole curve!! This is what the \"bias\" neuron can do.\n",
    "\n",
    "To see why this is important, imagine we are trying to fit a line by only having weights:\n",
    "\n",
    "$ y = w*x$\n",
    "\n",
    "This can of course only change the slope of the line. But we also need to shift the line, so we need an intercept or bias:\n",
    "\n",
    "$ y = w*x + b$\n",
    "\n",
    "\n",
    "Let's add this neuron to our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twoLayerBias(X,y,plotting=True):\n",
    "    # seed random numbers to get repeatable results\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # let's add ones to the data to model the bias\n",
    "    X=np.hstack((np.ones((X.shape[0],1)),X))\n",
    "    # initialize weights randomly (mean 0)\n",
    "    # the number of weights is determined by the number\n",
    "    # of columns in both the input data X (but remember we\n",
    "    # have the bias). We still connect of course to the \n",
    "    # output dimensions determined by the data y \n",
    "    syn0 = 2*np.random.random((X.shape[1],y.shape[1])) - 1\n",
    "\n",
    "    # maximum iteration\n",
    "    maxIter = 10000\n",
    "\n",
    "    # store errors\n",
    "    l1ErrorArray = np.zeros(maxIter)\n",
    "\n",
    "    # do some iterations\n",
    "    for it in np.arange(maxIter):\n",
    "\n",
    "        # forward propagation: we put in our pattern\n",
    "        # as layer \"0\" and then push it through the\n",
    "        # activation function to get the output of\n",
    "        # the layer\n",
    "        l0 = X\n",
    "        l1 = sigmoid(np.dot(l0,syn0))\n",
    "\n",
    "        # evaluate the error of the layer\n",
    "        l1Error = y - l1\n",
    "\n",
    "        # evaluate the summed squared error\n",
    "        l1ErrorArray[it] = np.sum(l1Error*l1Error)\n",
    "\n",
    "        # print out the summed squared error sometimes\n",
    "        if (it%1000==0):\n",
    "            sys.stdout.write(\"Iteration {:d}: error = {:f}\\r\".format(it,l1ErrorArray[it]))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # the error determines the amount we need\n",
    "        # to move along the derivative\n",
    "        l1Delta = l1Error * dsigmoid(l1)\n",
    "\n",
    "        # the weight update is the dot product between\n",
    "        # the pattern input and the correction amount \n",
    "        syn0 += np.dot(l0.T,l1Delta)\n",
    "\n",
    "    print(\"output after training is:\\n\",l1)\n",
    "    if (plotting):\n",
    "        fig,ax = plt.subplots(figsize=(8,6))\n",
    "        plt.plot(l1ErrorArray)\n",
    "    return(syn0,l1ErrorArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,0],\n",
    "                [1,1,0],\n",
    "                [0,1,0],\n",
    "                [0,0,0]])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# here is a simple, logical combination:\n",
    "y = np.logical_or(X[:,0],np.logical_and(X[:,1],X[:,2]))\n",
    "print(y)\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors)=twoLayerBias(X,y.reshape(-1,1))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that works.\n",
    "\n",
    "So let's try something very different. Let's go back to our beautiful IRIS dataset and try to categorize using a two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = iris.data[:100]\n",
    "    \n",
    "# this is the target state we want to have\n",
    "y = iris.target[:100]\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors)=twoLayerBias(X,y.reshape(-1,1))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugh. That did not go so well. Somehow, the network did not learn. Instead the error goes pretty much immediately to high values. What went wrong?\n",
    "\n",
    "The problem is in the input values. When using the gradient for updating the weights, the gradient can become too big and the network \"overshoots\". The best solution is to:\n",
    "\n",
    "* normalize the data: `X = (X-X.mean(axis=0))/X.std(axis=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = iris.data[:100]\n",
    "    \n",
    "# normalize the data\n",
    "X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "\n",
    "# this is the target state we want to have\n",
    "y = iris.target[:100]\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors)=twoLayerBias(X,y.reshape(-1,1))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. As we can see, the examples are correctly recognized and the weights that are given to the four dimensions are now different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking the neural network\n",
    "\n",
    "Let's return to our simple, 0/1 example and try a different training/testing combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1] ])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# note, that each row here corresponds to the\n",
    "# row in X - so we are trying to basically\n",
    "# learn two class labels from data here\n",
    "y = np.array([[1],\n",
    "              [1],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors)=twoLayerBias(X,y)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch. So that does not work. We have left the realm of simple correlations and the output space is a weird, highly non-linear combination of the inputs.\n",
    "\n",
    "But this is a sigmoid-network, so can our network even learn non-linear things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1] ])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# here is an explictly non-linear function:\n",
    "y = (X[:,0]+(X[:,1]-X[:,2])/(X[:,1]+0.4))\n",
    "print(y)\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors)=twoLayerBias(X,y.reshape(-1,1))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently not. But wait!\n",
    "\n",
    "If we look at the \"y\" target values, we can see that they are well outside the 0,1 range of the network inputs. Since the activation function itself is normalized between 0 and 1, we of course should normalize our output to be between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1] ])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# here is an explictly non-linear function:\n",
    "y = (X[:,0]+(X[:,1]-X[:,2])/(X[:,1]+0.4))\n",
    "print(y)\n",
    "y = (y-y.min())/(y.max()-y.min())\n",
    "print(y)\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors)=twoLayerBias(X,y.reshape(-1,1))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, still, the example from above cannot apparently be learned. This is because even though we have non-linear activation, the target vectors are far outside the \"span\" of the input examples.\n",
    "\n",
    "The solution? \n",
    "\n",
    "More layers!\n",
    "\n",
    "## Three-layer network\n",
    "\n",
    "Let's add a \"hidden\" layer to our network. We also add a bias node to the input layer of the network in order to be able to shift the outputs around.\n",
    "\n",
    "Finally, since multi-layer networks use a gradient descent via backpropagation, we are in danger of getting stuck in local minima. One way to avoid this is to use a \"learning rate\" that is used to dampen the step size update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def threeLayerBias(X,y,numHidden=5,alpha=0.02,plotting=True):\n",
    "\n",
    "    # seed random numbers to get repeatable results\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # let's add ones to the data to model the bias\n",
    "    X=np.hstack((np.ones((X.shape[0],1)),X))\n",
    "    \n",
    "    # initialize weights randomly (mean 0)\n",
    "    syn0 = 2*np.random.random((X.shape[1],numHidden)) - 1\n",
    "    syn1 = 2*np.random.random((numHidden,y.shape[1])) - 1\n",
    "\n",
    "    # maximum iteration\n",
    "    maxIter = 40000\n",
    "\n",
    "    # store errors\n",
    "    l2ErrorArray = np.zeros(maxIter)\n",
    "\n",
    "    # do some iterations\n",
    "    for it in np.arange(maxIter):\n",
    "\n",
    "        # forward propagation: we put in our pattern\n",
    "        # as layer \"0\" and then push it through the\n",
    "        # activation function to get the output of\n",
    "        # the layer\n",
    "        l0 = X\n",
    "        l1 = sigmoid(np.dot(l0,syn0))\n",
    "        l2 = sigmoid(np.dot(l1,syn1))\n",
    "\n",
    "        # evaluate the error of the layer\n",
    "        l2Error = y - l2\n",
    "\n",
    "        l2ErrorArray[it] = np.sum(l2Error*l2Error)\n",
    "\n",
    "        if (it%1000==0):\n",
    "            sys.stdout.write(\"Iteration {:d}: error = {:f}\\r\".format(it,l2ErrorArray[it]))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # the error determines the amount we need\n",
    "        # to move along the derivative\n",
    "        # to \"regularize\" this further, we multiply by the \n",
    "        # learning rate alpha\n",
    "\n",
    "        l2Delta = l2Error*dsigmoid(l2)\n",
    "        l1Delta = l2Delta.dot(syn1.T)*dsigmoid(l1)\n",
    "\n",
    "        # the weight update is the dot product between\n",
    "        # the pattern input and the correction amount\n",
    "        # this is moderated by the \"learning rate\" alpha\n",
    "        syn1 += alpha*np.dot(l1.T,l2Delta)\n",
    "        syn0 += alpha*np.dot(X.T,l1Delta)\n",
    "\n",
    "    print(\"output after training is:\\n\",l2)\n",
    "\n",
    "    if (plotting):\n",
    "        fig,ax = plt.subplots(figsize=(8,6))\n",
    "        plt.plot(l2ErrorArray)\n",
    "    return(syn1,l2ErrorArray,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = np.array([  [0,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1],\n",
    "                [0,1,1] ])\n",
    "    \n",
    "# this is the target state we want to have, \n",
    "# our input is three numbers, our output is\n",
    "# one number\n",
    "# note, that each row here corresponds to the\n",
    "# row in X - so we are trying to basically\n",
    "# learn two class labels from data here\n",
    "y = np.array([[1],\n",
    "              [1],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors,_)=threeLayerBias(X,y,3,1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now this one works. You can also clearly see the location, where the optimization kicked in to do another round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# these are the inputs to our neural network\n",
    "# each row is one training example\n",
    "X = iris.data\n",
    "X = (X-X.mean(axis=0))/X.std(axis=0)    \n",
    "# this is the target state we want to have\n",
    "y = iris.target/2\n",
    "\n",
    "# let's call our function and do the training:\n",
    "(weights,errors,pred)=threeLayerBias(X,y.reshape(-1,1),5,1)\n",
    "print(weights)\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(pred)\n",
    "plt.plot(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
